<!DOCTYPE HTML>
<html lang="en-gb" class="no-js">
    <head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer International Publishing"/>
    <meta name="citation_title" content="Deep learning applications and challenges in big data analytics"/>
    <meta name="citation_doi" content="10.1186/s40537-014-0007-7"/>
    <meta name="citation_language" content="en"/>
    <meta name="citation_abstract_html_url" content="https://link.springer.com/article/10.1186/s40537-014-0007-7"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1186/s40537-014-0007-7"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1186%2Fs40537-014-0007-7.pdf"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1186/s40537-014-0007-7&amp;api_key="/>
    <meta name="citation_firstpage" content="1"/>
    <meta name="citation_author" content="Maryam M Najafabadi"/>
    <meta name="citation_author_institution" content="Florida Atlantic University"/>
    <meta name="citation_author" content="Flavio Villanustre"/>
    <meta name="citation_author_institution" content="LexisNexis Business Information Solutions"/>
    <meta name="citation_author" content="Taghi M Khoshgoftaar"/>
    <meta name="citation_author_institution" content="Florida Atlantic University"/>
    <meta name="citation_author" content="Naeem Seliya"/>
    <meta name="citation_author_institution" content="Florida Atlantic University"/>
    <meta name="citation_author" content="Randall Wald"/>
    <meta name="citation_author_institution" content="Florida Atlantic University"/>
    <meta name="citation_author_email" content="rwald1@fau.edu"/>
    <meta name="citation_author" content="Edin Muharemagic"/>
    <meta name="citation_author_institution" content="LexisNexis Business Information Solutions"/>
    <meta name="dc.identifier" content="10.1186/s40537-014-0007-7"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="description" content="Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of..."/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Deep learning applications and challenges in big data analytics"/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/40537/2/1.jpg"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:site" content="SpringerLink"/>
    <meta name="twitter:description" content="Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of..."/>
    <meta name="citation_journal_title" content="Journal of Big Data"/>
    <meta name="citation_journal_abbrev" content="Journal of Big Data"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_issn" content="2196-1115"/>
    <meta name="citation_online_date" content="2015/02/24"/>
    <meta name="citation_cover_date" content="2015/12/01"/>
    <meta name="citation_article_type" content="Research"/>
    <meta property="og:title" content="Deep learning applications and challenges in big data analytics"/>
    <meta property="og:type" content="Article"/>
    <meta property="og:url" content="https://link.springer.com/article/10.1186/s40537-014-0007-7"/>
    <meta property="og:image" content="https://static-content.springer.com/cover/journal/40537/2/1.jpg"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:description" content="Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of..."/>

        <title>Deep learning applications and challenges in big data analytics | SpringerLink</title>
        <link rel="canonical" href="https://link.springer.com/article/10.1186/s40537-014-0007-7"/>
        <link rel="shortcut icon" href="/springerlink-static/1645515599/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16 32x32 48x48" href="/springerlink-static/1645515599/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="/springerlink-static/1645515599/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="/springerlink-static/1645515599/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="/springerlink-static/1645515599/images/favicon/favicon-48x48.png">
<link rel="apple-touch-icon" href="/springerlink-static/1645515599/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon" sizes="72x72" href="/springerlink-static/1645515599/images/favicon/ic_launcher_hdpi.png">
<link rel="apple-touch-icon" sizes="76x76" href="/springerlink-static/1645515599/images/favicon/app-icon-ipad.png">
<link rel="apple-touch-icon" sizes="114x114" href="/springerlink-static/1645515599/images/favicon/app-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/springerlink-static/1645515599/images/favicon/app-icon-iphone@2x.png">
<link rel="apple-touch-icon" sizes="144x144" href="/springerlink-static/1645515599/images/favicon/ic_launcher_xxhdpi.png">
<link rel="apple-touch-icon" sizes="152x152" href="/springerlink-static/1645515599/images/favicon/app-icon-ipad@2x.png">
<link rel="apple-touch-icon" sizes="180x180" href="/springerlink-static/1645515599/images/favicon/app-icon-iphone@3x.png">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/1645515599/images/favicon/ic_launcher_xxhdpi.png">
        <link rel="dns-prefetch" href="//fonts.gstatic.com">
<link rel="dns-prefetch" href="//fonts.googleapis.com">
<link rel="dns-prefetch" href="//google-analytics.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagservices.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">
<link rel="dns-prefetch" href="//static-content.springer.com">
        <link rel="stylesheet" href="/springerlink-static/1645515599/css/basic.css" media="screen">
<link rel="stylesheet" href="/springerlink-static/1645515599/css/styles.css" class="js-ctm" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
<link rel="stylesheet" href="/springerlink-static/1645515599/css/print.css" media="print">


            <script type="text/javascript">
        window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
        var dataLayer = [{
                'GA Key':"UA-26408784-1",
                'Features':["leaderboardadverts","eventtracker"],
                'Event Category':"Article",
                'Open Access':"Y",
                'Labs':"Y",
                'DOI':"10.1186/s40537-014-0007-7",
                'VG Wort Identifier':"vgzm.415900-10.1186-s40537-014-0007-7",
                'HasAccess':"Y",
                'Full HTML':"Y",
                'Has Body':"Y",
                'Static Hash':"1645515599",
                'Has Preview':"N",
                'user':{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},
                'content':{"serial":{"eissn":"2196-1115","pissn":""},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"4":"Computational Science and Engineering","5":"Mathematical Applications in Computer Science","6":"Communications Engineering, Networks","1":"Database Management","2":"Information Storage and Retrieval","3":"Data Mining and Knowledge Discovery"},"secondarySubjectCodes":{"4":"M14026","5":"M13110","6":"T24035","1":"I18024","2":"I18032","3":"I18030"}},"sucode":""}},
                'Access Type':"subscription",
                'Page':"article",
                'Bpids':"",
                'Bpnames':"",
                'SubjectCodes':"SCI, SCI18024, SCI18032, SCI18030, SCM14026, SCM13110, SCT24035",
                'session':{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},
                'eventTrackerBaseUrl':"https://event-tracker.springernature.com",
                'Keywords':"Deep learning, Big data",
                'Country':"US",
                'Journal Id':"40537",
                'Journal Title':"Journal of Big Data",

                    'doi': "10.1186-s40537-014-0007-7",
                    'kwrd': ["Deep_learning","Big_data"],
                    'pmc': ["I","I18024","I18032","I18030","M14026","M13110","T24035"],
                    'BPID': ["1"],
                    'ksg': Krux.segments,
                    'kuid': Krux.uid,

        }];
    </script>

<script type="text/javascript" src="/springerlink-static/1645515599/js/jquery-3.3.1.min.js"></script>

        <script type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>

<script type="text/javascript">
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

    </head>
    <body>
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <div class="skip-to">
    <a class="skip-to__link pseudo-focus" href="#main-content">Skip to main content</a>
        <a class="skip-to__link skip-to__link--contents pseudo-focus" href="#article-contents">Skip to sections</a>
</div>
        <div class="page-wrapper">
            <noscript>
    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>
                    <div id="leaderboard" class="leaderboard u-hide" data-component="SpringerLink.GoogleAds" data-namespace="leaderboard">
            <div class="leaderboard__wrapper">
                <p class="leaderboard__label">Advertisement</p>
                <button class="leaderboard__hide" title="Hide this advertisement">Hide</button>
                <div id="doubleclick-leaderboard-ad" class="leaderboard__ad u-pt-24"></div>
            </div>
        </div>

                <header id="header" class="header u-interface">
        <div class="header__content">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="/" title="Go to homepage">
                <div class="u-screenreader-only">SpringerLink</div>
    <svg class="site-logo__springer" width="148" height="30" role="img" focusable="false" aria-hidden="true">
        <image width="148" height="30" alt="" src="/springerlink-static/1645515599/images/png/springerlink.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/1645515599/images/svg/springerlink.svg"></image>
    </svg>

    </a>


                    <nav id="search-container" class="u-inline-block">
                        <div class="search">
                            <div class="search__content">
                                <form class="u-form-single-input" action="/search" method="get" role="search">
    <label for="search-springerlink">Search SpringerLink</label>
    <div class="u-relative">
        <input id="search-springerlink" name="query" type="text" autocomplete="off" value="">
        <input class="u-hide-text" type="submit" value="Submit" title="Submit">
        <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
            <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"/>
        </svg>
    </div>
</form>
                            </div>
                        </div>
                    </nav>

                    <nav class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SV.Menu" data-title="Navigation menu" data-text="Menu">
            <li>
                <a href="/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1186%2Fs40537-014-0007-7">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul>
    </div> 
</nav> 
            </div>

        </div>
    </header>

            

            <main id="main-content" class="main-wrapper" tabindex="-1">
                <div class="main-container uptodate-recommendations-off">
                    <aside class="main-sidebar-left">
                        <div class="main-sidebar-left__content">
                            <div class="cover-image test-cover" itemscope>
                                    <a class="test-cover-link" href="/journal/40537">
        <span class="u-screenreader-only">Journal of Big Data</span>
        <img class="test-cover-image" src="https://media.springernature.com/w306/springer-static/cover/journal/40537/2/1.jpg" itemprop="image" alt=""/>
    </a>


                            </div>
                        </div>
                    </aside>
                    <div class="main-body" data-role="NavigationContainer">
                                <div class="cta-button-container cta-button-container--top cta-button-container--stacked u-mb-16 u-hide-two-col">
                    <div>
            <a href="/content/pdf/10.1186%2Fs40537-014-0007-7.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

        </div>



                        <article class="main-body__content">
                            <div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div id="enumeration" class="enumeration"><p><a href="/journal/40537" title="Journal of Big Data"><span class="JournalTitle">Journal of Big Data</span></a></p><p class="icon--meta-keyline-before"><span class="ArticleCitation_Year"><time datetime="2015-12">December 2015</time>, </span><span class="ArticleCitation_Volume">2:1</span><span class="u-inline-block u-ml-4"> | <a class="gtm-cite-link" href="#citeas">Cite as</a></span></p></div><div class="MainTitleSection"><h1 class="ArticleTitle" lang="en">Deep learning applications and challenges in big data analytics</h1></div><div class="authors u-clearfix" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><span>Authors</span></li><li><a href="#authorsandaffiliations" class="gtm-tab-authorsandaffiliations">Authors and affiliations</a></li></ul><div class="authors__list" data-role="AuthorsList"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Maryam M Najafabadi</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Flavio Villanustre</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Taghi M Khoshgoftaar</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Naeem Seliya</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Randall Wald</span><span class="author-information"><span class="authors__contact"><a href="mailto:rwald1@fau.edu" title="rwald1@fau.edu" itemprop="email" class="gtm-email-author"><img src="/springerlink-static/images/svg/email.svg" height="24" width="24" alt="Email author" /></a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Edin Muharemagic</span></li></ul></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><div id="open-choice-icon" class="open-access u-text-separator-after-micro">Open Access</div><span class="test-render-category">Research</span></span><div class="article-dates"><span class="article-dates__label">First Online: </span><span class="article-dates__first-online"><time datetime="2015-02-24">24 February 2015</time></span></div></div><div class="main-context__column">    <ul id="book-metrics" class="article-metrics u-sansSerif">
            <li class="article-metrics__item">
                    <a class="article-metrics__link gtm-socialmediamentions-count" href="http://www.altmetric.com/details.php?citation_id&#x3D;3762160&amp;domain&#x3D;link.springer.com" target="_blank" rel="noopener"
                       title="Visit Altmetric for full social mention details" id="socialmediamentions-link">
                            <span id="socialmediamentions-count-number" class="test-metric-count c-button-circle gtm-socialmediamentions-count">109</span>
                       <span class="test-metric-name article-metrics__label gtm-socialmediamentions-count">Shares</span>
                    </a>
            </li>
            <li class="article-metrics__item">
                     <span class="article-metrics__views">141k</span>
                     <span class="article-metrics__label">Downloads</span>
            </li>
            <li class="article-metrics__item">
                    <a class="article-metrics__link gtm-citations-count" href="https://citations.springer.com/item?doi&#x3D;10.1186/s40537-014-0007-7" target="_blank" rel="noopener"
                       title="Visit Springer Citations for full citation details" id="citations-link">
                            <span id="citations-count-number" class="test-metric-count c-button-circle gtm-citations-count">197</span>
                       <span class="test-metric-name article-metrics__label gtm-citations-count">Citations</span>
                    </a>
            </li>
    </ul>
</div></div></div><section class="Abstract" id="Abs1" tabindex="-1" lang="en"><h2 class="Heading">Abstract</h2><p class="Para">Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.</p></section><div class="KeywordGroup" lang="en"><h2 class="Heading">Keywords</h2><span class="Keyword">Deep learning </span><span class="Keyword">Big data </span></div><div class="article-actions--inline" id="article-actions--inline" data-component="article-actions--inline"></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Introduction</h2><div class="content"><p class="Para">The general focus of machine learning is the representation of the input data and generalization of the learnt patterns for use on future unseen data. The goodness of the data representation has a large impact on the performance of machine learners on the data: a poor data representation is likely to reduce the performance of even an advanced, complex machine learner, while a good data representation can lead to high performance for a relatively simpler machine learner. Thus, feature engineering, which focuses on constructing features and data representations from raw data [<span class="CitationRef"><a href="#CR1">1</a></span>], is an important element of machine learning. Feature engineering consumes a large portion of the effort in a machine learning task, and is typically quite domain specific and involves considerable human input. For example, the Histogram of Oriented Gradients (HOG) [<span class="CitationRef"><a href="#CR2">2</a></span>] and Scale Invariant Feature Transform (SIFT) [<span class="CitationRef"><a href="#CR3">3</a></span>] are popular feature engineering algorithms developed specifically for the computer vision domain. Performing feature engineering in a more automated and general fashion would be a major breakthrough in machine learning as this would allow practitioners to automatically extract such features without direct human input.</p><p class="Para">Deep Learning algorithms are one promising avenue of research into the automated extraction of complex data representations (features) at high levels of abstraction. Such algorithms develop a layered, hierarchical architecture of learning and representing data, where higher-level (more abstract) features are defined in terms of lower-level (less abstract) features. The hierarchical learning architecture of Deep Learning algorithms is motivated by artificial intelligence emulating the deep, layered learning process of the primary sensorial areas of the neocortex in the human brain, which automatically extracts features and abstractions from the underlying data [<span class="CitationRef"><a href="#CR4">4</a></span>]-[<span class="CitationRef"><a href="#CR6">6</a></span>]. Deep Learning algorithms are quite beneficial when dealing with learning from large amounts of unsupervised data, and typically learn data representations in a greedy layer-wise fashion [<span class="CitationRef"><a href="#CR7">7</a></span>],[<span class="CitationRef"><a href="#CR8">8</a></span>]. Empirical studies have demonstrated that data representations obtained from stacking up non-linear feature extractors (as in Deep Learning) often yield better machine learning results, e.g., improved classification modeling [<span class="CitationRef"><a href="#CR9">9</a></span>], better quality of generated samples by generative probabilistic models [<span class="CitationRef"><a href="#CR10">10</a></span>], and the invariant property of data representations [<span class="CitationRef"><a href="#CR11">11</a></span>]. Deep Learning solutions have yielded outstanding results in different machine learning applications, including speech recognition [<span class="CitationRef"><a href="#CR12">12</a></span>]-[<span class="CitationRef"><a href="#CR16">16</a></span>], computer vision [<span class="CitationRef"><a href="#CR7">7</a></span>],[<span class="CitationRef"><a href="#CR8">8</a></span>],[<span class="CitationRef"><a href="#CR17">17</a></span>], and natural language processing [<span class="CitationRef"><a href="#CR18">18</a></span>]-[<span class="CitationRef"><a href="#CR20">20</a></span>]. A more detailed overview of Deep Learning is presented in Section “Deep learning in data mining and machine learning”.</p><p class="Para">Big Data represents the general realm of problems and techniques used for application domains that collect and maintain massive volumes of raw data for domain-specific data analysis. Modern data-intensive technologies as well as increased computational and data storage resources have contributed heavily to the development of Big Data science [<span class="CitationRef"><a href="#CR21">21</a></span>]. Technology based companies such as Google, Yahoo, Microsoft, and Amazon have collected and maintained data that is measured in exabyte proportions or larger. Moreover, social media organizations such as Facebook, YouTube, and Twitter have billions of users that constantly generate a very large quantity of data. Various organizations have invested in developing products using Big Data Analytics to addressing their monitoring, experimentation, data analysis, simulations, and other knowledge and business needs [<span class="CitationRef"><a href="#CR22">22</a></span>], making it a central topic in data science research.</p><p class="Para">Mining and extracting meaningful patterns from massive input data for decision-making, prediction, and other inferencing is at the core of Big Data Analytics. In addition to analyzing massive volumes of data, Big Data Analytics poses other unique challenges for machine learning and data analysis, including format variation of the raw data, fast-moving streaming data, trustworthiness of the data analysis, highly distributed input sources, noisy and poor quality data, high dimensionality, scalability of algorithms, imbalanced input data, unsupervised and un-categorized data, limited supervised/labeled data, etc. Adequate data storage, data indexing/tagging, and fast information retrieval are other key problems in Big Data Analytics. Consequently, innovative data analysis and data management solutions are warranted when working with Big Data. For example, in a recent work we examined the high-dimensionality of bioinformatics domain data and investigated feature selection techniques to address the problem [<span class="CitationRef"><a href="#CR23">23</a></span>]. A more detailed overview of Big Data Analytics is presented in Section “Big data analytics”.</p><p class="Para">The knowledge learnt from (and made available by) Deep Learning algorithms has been largely untapped in the context of Big Data Analytics. Certain Big Data domains, such as computer vision [<span class="CitationRef"><a href="#CR17">17</a></span>] and speech recognition [<span class="CitationRef"><a href="#CR13">13</a></span>], have seen the application of Deep Learning largely to improve classification modeling results. The ability of Deep Learning to extract high-level, complex abstractions and data representations from large volumes of data, especially unsupervised data, makes it attractive as a valuable tool for Big Data Analtyics. More specifically, Big Data problems such as semantic indexing, data tagging, fast information retrieval, and discriminative modeling can be better addressed with the aid of Deep Learning. More traditional machine learning and feature engineering algorithms are not efficient enough to extract the complex and non-linear patterns generally observed in Big Data. By extracting such features, Deep Learning enables the use of relatively simpler linear models for Big Data analysis tasks, such as classification and prediction, which is important when developing models to deal with the scale of Big Data. The novelty of this study is that it explores the application of Deep Learning algorithms for key problems in Big Data Analytics, motivating further targeted research by experts in these twofields.</p><p class="Para">The paper focuses on two key topics: (1) how Deep Learning can assist with specific problems in Big Data Analytics, and (2) how specific areas of Deep Learning can be improved to reflect certain challenges associated with Big Data Analytics. With respect to the first topic, we explore the application of Deep Learning for specific Big Data Analytics, including learning from massive volumes of data, semantic indexing, discriminative tasks, and data tagging. Our investigation regarding the second topic focuses on specific challenges Deep Learning faces due to existing problems in Big Data Analytics, including learning from streaming data, dealing with high dimensionality of data, scalability of models, and distributed and parallel computing. We conclude by identifying important future areas needing innovation in Deep Learning for Big Data Analytics, including data sampling for generating useful high-level abstractions, domain (data distribution) adaption, defining criteria for extracting good data representations for discriminative and indexing tasks, semi-supervised learning, and active learning.</p><p class="Para">The remainder of the paper is structured as follows: Section “Deep learning in data mining and machine learning” presents an overview of Deep Learning for data analysis in data mining and machine learning; Section “Big data analytics” presents an overview of Big Data Analytics, including key characteristics of Big Data and identifying specific data analysis problems faced in Big Data Analytics; Section “Applications of deep learning in big data analytics” presents a targeted survey of works investigating Deep Learning based solutions for data analysis, and discusses how Deep Learning can be applied for Big Data Analytics problems; Section “Deep learning challenges in big data analytics” discusses some challenges faced by Deep Learning experts due to specific data analysis needs of Big Data; Section “Future work on deep learning in big data analytics” presents our insights into further works that are necessary for extending the application of Deep Learning in Big Data, and poses important questions to domain experts; and in Section “Conclusion” we reiterate the focus of the paper and summarize the workpresented.</p></div></section><section id="Sec2" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Deep learning in data mining and machine learning</h2><div class="content"><p class="Para">The main concept in deep leaning algorithms is automating the extraction of representations (abstractions) from the data [<span class="CitationRef"><a href="#CR5">5</a></span>],[<span class="CitationRef"><a href="#CR24">24</a></span>],[<span class="CitationRef"><a href="#CR25">25</a></span>]. Deep learning algorithms use a huge amount of unsupervised data to automatically extract complex representation. These algorithms are largely motivated by the field of artificial intelligence, which has the general goal of emulating the human brain’s ability to observe, analyze, learn, and make decisions, especially for extremely complex problems. Work pertaining to these complex challenges has been a key motivation behind Deep Learning algorithms which strive to emulate the hierarchical learning approach of the human brain. Models based on shallow learning architectures such as decision trees, support vector machines, and case-based reasoning may fall short when attempting to extract useful information from complex structures and relationships in the input corpus. In contrast, Deep Learning architectures have the capability to generalize in non-local and global ways, generating learning patterns and relationships beyond immediate neighbors in the data [<span class="CitationRef"><a href="#CR4">4</a></span>]. Deep learning is in fact an important step toward artificial intelligence. It not only provides complex representations of data which are suitable for AI tasks but also makes the machines independent of human knowledge which is the ultimate goal of AI. It extracts representations directly from unsupervised data without human interference.</p><p class="Para">A key concept underlying Deep Learning methods is distributed representations of the data, in which a large number of possible configurations of the abstract features of the input data are feasible, allowing for a compact representation of each sample and leading to a richer generalization. The number of possible configurations is exponentially related to the number of extracted abstract features. Noting that the observed data was generated through interactions of several known/unknown factors, and thus when a data pattern is obtained through some configurations of learnt factors, additional (unseen) data patterns can likely be described through new configurations of the learnt factors and patterns[<span class="CitationRef"><a href="#CR5">5</a></span>],[<span class="CitationRef"><a href="#CR24">24</a></span>]. Compared to learning based on local generalizations, the number of patterns that can be obtained using a distributed representation scales quickly with the number of learnt factors.</p><p class="Para">Deep learning algorithms lead to abstract representations because more abstract representations are often constructed based on less abstract ones. An important advantage of more abstract representations is that they can be invariant to the local changes in the input data. Learning such invariant features is an ongoing major goal in pattern recognition (for example learning features that are invariant to the face orientation in a face recognition task). Beyond being invariant such representations can also disentangle the factors of variation in data. The real data used in AI-related tasks mostly arise from complicated interactions of many sources. For example an image is composed of different sources of variations such a light, object shapes, and object materials. The abstract representations provided by deep learning algorithms can separate the different sources of variations in data.</p><p class="Para">Deep learning algorithms are actually Deep architectures of consecutive layers. Each layer applies a nonlinear transformation on its input and provides a representation in its output. The objective is to learn a complicated and abstract representation of the data in a hierarchical manner by passing the data through multiple transformation layers. The sensory data (for example pixels in an image) is fed to the first layer. Consequently the output of each layer is provided as input to its next layer.</p><p class="Para">Stacking up the nonlinear transformation layers is the basic idea in deep learning algorithms. The more layers the data goes through in the deep architecture, the more complicated the nonlinear transformations which are constructed. These transformations represent the data, so Deep Learning can be considered as special case of representation learning algorithms which learn representations of the data in a Deep Architecture with multiple levels of representations. The achieved final representation is a highly non-linear function of the input data.</p><p class="Para">It is important to note that the transformations in the layers of deep architecture are non-linear transformations which try to extract underlying explanatory factors in the data. One cannot use a linear transformation like PCA as the transformation algorithms in the layers of the deep structure because the compositions of linear transformations yield another linear transformation. Therefore, there would be no point in having a deep architecture. For example by providing some face images to the Deep Learning algorithm, at the first layer it can learn the edges in different orientations; in the second layer it composes these edges to learn more complex features like different parts of a face such as lips, noses and eyes. In the third layer it composes these features to learn even more complex feature like face shapes of different persons. These final representations can be used as feature in applications of face recognition. This example is provided to simply explain in an understandable way how a deep learning algorithm finds more abstract and complicated representations of data by composing representations acquired in a hierarchical architecture. However, it must be considered that deep learning algorithms do not necessarily attempt to construct a pre-defined sequence of representations at each layer (such as edges, eyes, faces), but instead more generally perform non-linear transformations in different layers. These transformations tend to disentangle factors of variations in data. Translating this concept to appropriate training criteria is still one of the main open questions in deep learning algorithms [<span class="CitationRef"><a href="#CR5">5</a></span>].</p><p class="Para">The final representation of data constructed by the deep learning algorithm (output of the final layer) provides useful information from the data which can be used as features in building classifiers, or even can be used for data indexing and other applications which are more efficient when using abstract representations of data rather than high dimensional sensory data.</p><p class="Para">Learning the parameters in a deep architecture is a difficult optimization task, such as learning the parameters in neural networks with many hidden layers. In 2006 Hinton proposed learning deep architectures in an unsupervised greedy layer-wise learning manner [<span class="CitationRef"><a href="#CR7">7</a></span>]. At the beginning the sensory data is fed as learning data to the first layer. The first layer is then trained based on this data, and the output of the first layer (the first level of learnt representations) is provided as learning data to the second layer. Such iteration is done until the desired number of layers is obtained. At this point the deep network is trained. The representations learnt on the last layer can be used for different tasks. If the task is a classification task usually another supervised layer is put on top of the last layer and its parameters are learnt (either randomly or by using supervised data and keeping the rest of the network fixed). At the end the whole network is fine-tuned by providing supervised data to it.</p><p class="Para">Here we explain two fundamental building blocks, unsupervised single layer learning algorithms which are used to construct deeper models: Autoencoders and Restricted Boltzmann Machines (RBMs). These are often employed in tandem to construct stacked Autoencoders [<span class="CitationRef"><a href="#CR8">8</a></span>],[<span class="CitationRef"><a href="#CR26">26</a></span>] and Deep belief networks [<span class="CitationRef"><a href="#CR7">7</a></span>], which are constructed by stacking up Autoencoders and Restricted Boltzmann Machines respectively. Autoencoders, also called autoassociators [<span class="CitationRef"><a href="#CR27">27</a></span>], are networks constructed of 3 layers: input, hidden and output. Autoencoders try to learn some representations of the input in the hidden layer in a way that makes it possible to reconstruct the input in the output layer based on these intermediate representations. Thus, the target output is the input itself. A basic Autoencoder learns its parameters by minimizing the reconstruction error. This minimization is usually done by stochastic gradient descent (much like what is done in Multilayer Perceptron). If the hidden layer is linear and the mean squared error is used as the reconstruction criteria, then the Autoencoder will learn the first k principle components of the data. Alternative strategies are proposed to make Autoencoders nonlinear which are appropriate to build deep networks as well as to extract meaningful representations of data rather than performing just as a dimensionality reduction method. Bengio et al. have called these methods “regularized Autoencoders” in [<span class="CitationRef"><a href="#CR5">5</a></span>], and we refer an interested reader to that paper for more details on algorithms.</p><p class="Para">Another unsupervised single layer learning algorithm which is used as a building block in constructing Deep Belief Networks is the Restricted Boltzmann machine (RBM). RBMs are most likely the most popular version of Boltzmann machine [<span class="CitationRef"><a href="#CR28">28</a></span>]. They contains one visible layer and one hidden layer. The restriction is that there is no interaction between the units of the same layer and the connections are solely between units from different layers. The Contrastive Divergence algorithm [<span class="CitationRef"><a href="#CR29">29</a></span>] has mostly been used to train the Boltzmann machine.</p></div></section><section id="Sec3" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Big data analytics</h2><div class="content"><p class="Para">Big Data generally refers to data that exceeds the typical storage, processing, and computing capacity of conventional databases and data analysis techniques. As a resource, Big Data requires tools and methods that can be applied to analyze and extract patterns from large-scale data. The rise of Big Data has been caused by increased data storage capabilities, increased computational processing power, and availability of increased volumes of data, which give organization more data than they have computing resources and technologies to process. In addition to the obvious great volumes of data, Big Data is also associated with other specific complexities, often referred to as the four Vs: Volume, Variety, Velocity, and Veracity [<span class="CitationRef"><a href="#CR22">22</a></span>],[<span class="CitationRef"><a href="#CR30">30</a></span>],[<span class="CitationRef"><a href="#CR31">31</a></span>]. We note that the aim of this section is not to extensively cover Big Data, but present a brief overview of its key concepts and challenges while keeping in mind that the use of Deep Learning in Big Data Analytics is the focus of this paper.</p><p class="Para">The unmanageable large Volume of data poses an immediate challenge to conventional computing environments and requires scalable storage and a distributed strategy to data querying and analysis. However, this large Volume of data is also a major positive feature of Big Data. Many companies, such as Facebook, Yahoo, Google, already have large amounts of data and have recently begun tapping into its benefits [<span class="CitationRef"><a href="#CR21">21</a></span>]. A general theme in Big Data systems is that the raw data is increasingly diverse and complex, consisting of largely un-categorized/unsupervised data along with perhaps a small quantity of categorized/supervised data. Working with the Variety among different data representations in a given repository poses unique challenges with Big Data, which requires Big Data preprocessing of unstructured data in order to extract structured/ordered representations of the data for human and/or downstream consumption. In today’s data-intensive technology era, data Velocity – the increasing rate at which data is collected and obtained – is just as important as the Volume and Variety characteristics of Big Data. While the possibility of data loss exists with streaming data if it is generally not immediately processed and analyzed, there is the option to save fast-moving data into bulk storage for batch processing at a later time. However, the practical importance of dealing with Velocity associated with Big Data is the quickness of the feedback loop, that is, process of translating data input into useable information. This is especially important in the case of time-sensitive information processing. Some companies such as Twitter, Yahoo, and IBM have developed products that address the analysis of streaming data [<span class="CitationRef"><a href="#CR22">22</a></span>]. Veracity in Big Data deals with the trustworthiness or usefulness of results obtained from data analysis, and brings to light the old adage “Garbage-In-Garbage-Out” for decision making based on Big Data Analytics. As the number of data sources and types increases, sustaining trust in Big Data Analytics presents a practical challenge.</p><p class="Para">Big Data Analytics faces a number of challenges beyond those implied by the four Vs. While not meant to be an exhaustive list, some key problem areas include: data quality and validation, data cleansing, feature engineering, high-dimensionality and data reduction, data representations and distributed data sources, data sampling, scalability of algorithms, data visualization, parallel and distributed data processing, real-time analysis and decision making, crowdsourcing and semantic input for improved data analysis, tracing and analyzing data provenance, data discovery and integration, parallel and distributed computing, exploratory data analysis and interpretation, integrating heterogenous data, and developing new models for massive data computation.</p></div></section><section id="Sec4" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Applications of deep learning in big data analytics</h2><div class="content"><p class="Para">As stated previously, Deep Learning algorithms extract meaningful abstract representations of the raw data through the use of an hierarchical multi-level learning approach, where in a higher-level more abstract and complex representations are learnt based on the less abstract concepts and representations in the lower level(s) of the learning hierarchy. While Deep Learning can be applied to learn from labeled data if it is available in sufficiently large amounts, it is primarily attractive for learning from large amounts of unlabeled/unsupervised data [<span class="CitationRef"><a href="#CR4">4</a></span>],[<span class="CitationRef"><a href="#CR5">5</a></span>],[<span class="CitationRef"><a href="#CR25">25</a></span>], making it attractive for extracting meaningful representations and patterns from Big Data.</p><p class="Para">Once the hierarchical data abstractions are learnt from unsupervised data with Deep Learning, more conventional discriminative models can be trained with the aid of relatively fewer supervised/labeled data points, where the labeled data is typically obtained through human/expert input. Deep Learning algorithms are shown to perform better at extracting non-local and global relationships and patterns in the data, compared to relatively shallow learning architectures [<span class="CitationRef"><a href="#CR4">4</a></span>]. Other useful characteristics of the learnt abstract representations by Deep Learning include: (1) relatively simple linear models can work effectively with the knowledge obtained from the more complex and more abstract data representations, (2) increased automation of data representation extraction from unsupervised data enables its broad application to different data types, such as image, textural, audio, etc., and (3) relational and semantic knowledge can be obtained at the higher levels of abstraction and representation of the raw data. While there are other useful aspects of Deep Learning based representations of data, the specific characteristics mentioned above are particularly important for Big Data Analytics.</p><p class="Para">Considering each of the four Vs of Big Data characteristics, i.e., Volume, Variety, Velocity, and Veracity, Deep Learning algorithms and architectures are more aptly suited to address issues related to Volume and Variety of Big Data Analytics. Deep Learning inherently exploits the availability of massive amounts of data, i.e. Volume in Big Data, where algorithms with shallow learning hierarchies fail to explore and understand the higher complexities of data patterns. Moreover, since Deep Learning deals with data abstraction and representations, it is quite likely suited for analyzing raw data presented in different formats and/or from different sources, i.e. Variety in Big Data, and may minimize need for input from human experts to extract features from every new data type observed in Big Data. While presenting different challenges for more conventional data analysis approaches, Big Data Analytics presents an important opportunity for developing novel algorithms and models to address specific issues related to Big Data. Deep Learning concepts provide one such solution venue for data analytics experts and practitioners. For example, the extracted representations by Deep Learning can be considered as a practical source of knowledge for decision-making, semantic indexing, information retrieval, and for other purposes in Big Data Analytics, and in addition, simple linear modeling techniques can be considered for Big Data Analytics when complex data is represented in higher forms of abstraction.</p><p class="Para">In the remainder of this section, we summarize some important works that have been performed in the field of Deep Learning algorithms and architectures, including semantic indexing, discriminative tasks, and data tagging. Our focus is that by presenting these works in Deep Learning, experts can observe the novel applicability of Deep Learning techniques in Big Data Analytics, particularly since some of the application domains in the works presented involve large scale data. Deep Learning algorithms are applicable to different kinds of input data; however, in this section we focus on its application on image, textual, and audio data.</p><section id="Sec5" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Semantic indexing</h3><p class="Para">A key task associated with Big Data Analytics is information retrieval [<span class="CitationRef"><a href="#CR21">21</a></span>]. Efficient storage and retrieval of information is a growing problem in Big Data, particularly since very large-scale quantities of data such as text, image, video, and audio are being collected and made available across various domains, e.g., social networks, security systems, shopping and marketing systems, defense systems, fraud detection, and cyber traffic monitoring. Previous strategies and solutions for information storage and retrieval are challenged by the massive volumes of data and different data representations, both associated with Big Data. In these systems, massive amounts of data are available that needs semantic indexing rather than being stored as data bit strings. Semantic indexing presents the data in a more efficient manner and makes it useful as a source for knowledge discovery and comprehension, for example by making search engines work more quickly and efficiently.</p><p class="Para">Instead of using raw input for data indexing, Deep Learning can be used to generate high-level abstract data representations which will be used for semantic indexing. These representations can reveal complex associations and factors (especially when the raw input was Big Data), leading to semantic knowledge and understanding. Data representations play an important role in the indexing of data, for example by allowing data points/instances with relatively similar representations to be stored closer to one another in memory, aiding in efficient information retrieval. It should be noted, however, that the high-level abstract data representations need to be meaningful and demonstrate relational and semantic association in order to actually confer a good semantic understanding and comprehension of the input.</p><p class="Para">While Deep Learning aids in providing a semantic and relational understanding of the data, a vector representation (corresponding to the extracted representations) of data instances would provide faster searching and information retrieval. More specifically, since the learnt complex data representations contain semantic and relational information instead of just raw bit data, they can directly be used for semantic indexing when each data point (for example a given text document) is presented by a vector representation, allowing for a vector-based comparison which is more efficient than comparing instances based directly on raw data. The data instances that have similar vector representations are likely to have similar semantic meaning. Thus, using vector representations of complex high-level data abstractions for indexing the data makes semantic indexing feasible. In the remainder of this section, we focus on document indexing based on knowledge gained from Deep Learning. However, the general idea of indexing based on data representations obtained from Deep Learning can be extended to other forms of data.</p><p class="Para">Document (or textual) representation is a key aspect in information retrieval for many domains. The goal of document representation is to create a representation that condenses specific and unique aspects of the document, e.g. document topic. Document retrieval and classification systems are largely based on word counts, representing the number of times each word occurs in the document. Various document retrieval schemas use such a strategy, e.g., TF-IDF [<span class="CitationRef"><a href="#CR32">32</a></span>] and BM25 [<span class="CitationRef"><a href="#CR33">33</a></span>]. Such document representation schemas consider individual words to be dimensions, with different dimensions being independent. In practice, it is often observed that the occurrence of words are highly correlated. Using Deep Learning techniques to extract meaningful data representations makes it possible to obtain semantic features from such high-dimensional textual data, which in turn also leads to the reduction of the dimensions of the document data representations.</p><p class="Para">Hinton et al. [<span class="CitationRef"><a href="#CR34">34</a></span>] describe a Deep Learning generative model to learn the binary codes for documents. The lowest layer of the Deep Learning network represents the word-count vector of the document which accounts as high-dimensional data, while the highest layer represents the learnt binary code of the document. Using 128-bit codes, the authors demonstrate that the binary codes of the documents that are semantically similar lay relatively closer in the Hamming space. The binary code of the documents can then be used for information retrieval. For each query document, its Hamming distance compared to all other documents in the data is computed and the top <em class="EmphasisTypeItalic ">D</em> similar documents are retrieved. Binary codes require relatively little storage space, and in addition they allow relatively quicker searches by using algorithms such as fast-bit counting to compute the Hamming distance between two binary codes. The authors conclude that using these binary codes for document retrieval is more accurate and faster than semantic-based analysis.</p><p class="Para">Deep Learning generative models can also be used to produce shorter binary codes by forcing the highest layer in the learning hierarchy to use a relatively small number of variables. These shorter binary codes can then simply be used as memory addresses. One word of memory is used to describe each document in such a way that a small Hamming-ball around that memory address contains semantically similar documents – such a technique is referred as “semantic hashing” [<span class="CitationRef"><a href="#CR35">35</a></span>]. Using such a strategy, one can perform information retrieval on a very large document set with the retrieval time being independent of the document set size. Techniques such as semantic hashing are quite attractive for information retrieval, because documents that are similar to the query document can be retrieved by finding all the memory addresses that differ from the memory address of the query document by a few bits. The authors demonstrate that “memory hashing” is much faster than locality-sensitive hashing, which is one of the fastest methods among existing algorithms. In addition, it is shown that by providing a document’s binary codes to algorithms such as TF-IDF instead of providing the entire document, a higher level of accuracy can be achieved. While Deep Learning generative models can have a relatively slow learning/training time for producing binary codes for document retrieval, the resulting knowledge yields fast inferences which is one major goal of Big Data Analytics. More specifically, producing the binary code for a new document requires just a few vector matrix computations performing a feed-forward pass through the encoder component of the Deep Learning network architecture.</p><p class="Para">To learn better representations and abstractions, one can use some supervised data in training the Deep Learning model. Ranzato et al. [<span class="CitationRef"><a href="#CR36">36</a></span>] present a study in which parameters of the Deep Learning model are learnt based on both supervised and unsupervised data. The advantages of such a strategy are that there is no need to completely label a large collection of data (as some unlabeled data is expected) and that the model has some prior knowledge (via the supervised data) to capture relevant class/label information in the data. In other words, the model is required to learn data representations that produce good reconstructions of the input in addition to providing good predictions of document class labels. The authors show that for learning compact representations, Deep Learning models are better than shallow learning models. The compact representations are efficient because they require fewer computations when used in indexing, and in addition, also need less storage capacity.</p><p class="Para">Google’s “word2vec” tool is another technique for automated extraction of semantic representations from Big Data. This tool takes a large-scale text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words, upon which the word vector file can be used as features in many Natural Language Processing (NLP) and machine learning applications. Miklov et al. [<span class="CitationRef"><a href="#CR37">37</a></span>] introduce techniques to learn high-quality word vectors from huge datasets with hundreds of millions of words (including some datasets containing 1.6 billion words), and with millions of distinct words in the vocabulary. They focus on artificial neural networks to learn the distributed representation of words. To train the network on such a massive dataset, the models are implemented on top of the large-scale distributed framework “DistBelief” [<span class="CitationRef"><a href="#CR38">38</a></span>]. The authors find that word vectors which are trained on massive amounts of data show subtle semantic relationships between words, such as a city and the country it belongs to – for example, Paris belongs to France and Berlin belongs to Germany. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval, and question response systems. For example, in a related work, Miklov et al. [<span class="CitationRef"><a href="#CR39">39</a></span>] demonstrate how word2vec can be applied for natural language translation.</p><p class="Para">Deep Learning algorithms make it possible to learn complex nonlinear representations between word occurrences, which allow the capture of high-level semantic aspects of the document (which could not normally be learned with linear models). Capturing these complex representations requires massive amounts of data for the input corpus, and producing labeled data from this massive input is a difficult task. With Deep Learning one can leverage unlabeled documents (unsupervised data) to have access to a much larger amount of input data, using a smaller amount of supervised data to improve the data representations and make them more related to the specific learning and inference tasks. The extracted data representations have been shown to be effective for retrieving documents, making them very useful for search engines.</p><p class="Para">Similar to textual data, Deep Learning can be used on other kinds of data to extract semantic representations from the input corpus, allowing for semantic indexing of that data. Given the relatively recent emergence of Deep Learning, additional work needs to be done on using its hierarchical learning strategy as a method for semantic indexing of Big Data. An remaining open question is what criteria is used to define “similar” when trying to extract data representations for indexing purposes (recall, data points that are semantically similar will have similar data representations in a specific distance space).</p></section><section id="Sec6" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Discriminative tasks and semantic tagging</h3><p class="Para">In performing discriminative tasks in Big Data Analytics one can use Deep Learning algorithms to extract complicated nonlinear features from the raw data, and then use simple linear models to perform discriminative tasks using the extracted features as input. This approach has two advantages: (1) extracting features with Deep Learning adds nonlinearity to the data analysis, associating the discriminative tasks closely to Artificial Intelligence, and (2) applying relatively simple linear analytical models on the extracted features is more computationally efficient, which is important for Big Data Analytics. The problem of developing efficient linear models for Big Data Analytics has been extensively investigated in the literature [<span class="CitationRef"><a href="#CR21">21</a></span>]. Hence, developing nonlinear features from massive amounts of input data allows the data analysts to benefit from the knowledge available through the massive amounts of data, by applying the learnt knowledge to simpler linear models for further analysis. This is an important benefit of using Deep Learning in Big Data Analytics, allowing practitioners to accomplish complicated tasks related to Artificial Intelligence, such as image comprehension, object recognition in images, etc., by using simpler models. Thus discriminative tasks are made relatively easier in Big Data Analytics with the aid of Deep Learning algorithms.</p><p class="Para">Discriminative analysis in Big Data Analytics can be the primary purpose of the data analysis, or it can be performed to conduct tagging (such as semantic tagging) on the data for the purpose of searching. For example, Li et al. [<span class="CitationRef"><a href="#CR40">40</a></span>] explore the Microsoft Research Audio Video Indexing System (MAVIS) that uses Deep Learning (with Artificial Neural Networks) based speech recognition technology to enable searching of audio and video files with speech. To converting digital audio and video signals into words, MAVIS automatically generates closed captions and keywords that can increase accessibility and discovery of audio and video files with speech content.</p><p class="Para">Considering the development of the Internet and the explosion of online users in recent years, there has been a very rapid increase in the size of digital image collections. These come from sources such as social networks, global positioning satellites, image sharing systems, medical imaging systems, military surveillance, and security systems. Google has explored and developed systems that provide image searches (e.g., the Google Images search service), including search systems that are only based on the image file name and document contents and do not consider/relate to the image content itself [<span class="CitationRef"><a href="#CR41">41</a></span>],[<span class="CitationRef"><a href="#CR42">42</a></span>]. Towards achieving artificial intelligence in providing improved image searches, practitioners should move beyond just the textual relationships of images, especially since textual representations of images are not always available in massive image collection repositories. Experts should strive towards collecting and organizing these massive image data collections, such that they can be browsed, searched, and retrieved more efficiently. To deal with large scale image data collections, one approach to consider is to automate the process of tagging images and extracting semantic information from the images. Deep Learning presents new frontiers towards constructing complicated representations for image and video data as relatively high levels of abstractions, which can then be used for image annotation and tagging that is useful for image indexing and retrieval. In the context of Big Data Analytics, here Deep Learning would aid in the discriminative task of semantic tagging of data.</p><p class="Para">Data tagging is another way to semantically index the input data corpus. However, it should not be confused with semantic indexing as discussed in the prior section. In semantic indexing, the focus is on using the Deep Learning abstract representations directly for data indexing purposes. Here the abstract data representations are considered as features for performing the discriminative task of data tagging. This tagging on data can also be used for data indexing as well, but the primary idea here is that Deep Leaning makes it possible to tag massive amounts of data by applying simple linear modeling methods on complicated features that were extracted by Deep Learning algorithms. The remainder of this section focuses largely on some results from using Deep Leaning for discriminative tasks that involve data tagging.</p><p class="Para">At the ImageNet Computer Vision Competition, Hinton et al. [<span class="CitationRef"><a href="#CR17">17</a></span>] demonstrated an approach using Deep Learning and Convolutional Neural Networks which outperformed other existing approaches for image object recognition. Using the ImageNet dataset, one of the largest for image object recognition, Hinton’s team showed the importance of Deep Learning for improving image searching. Dean et al. [<span class="CitationRef"><a href="#CR38">38</a></span>] demonstrated further success on ImageNet by using a similar Deep Learning modeling approach with a large-scale software infrastructure for training an artificial neural network.</p><p class="Para">Some other approaches have been tried for learning and extracting features from unlabeled image data, include Restricted Boltzmann Machines (RBMs) [<span class="CitationRef"><a href="#CR7">7</a></span>], autoencoders [<span class="CitationRef"><a href="#CR26">26</a></span>], and sparse coding [<span class="CitationRef"><a href="#CR43">43</a></span>]. However, these were only able to extract low-level features, such as edge and blob detection. Deep Learning can also be used to build very high-level features for image detection. For example, Google and Stanford formulated a very large deep neural network that was able to learn very high-level features, such as face detection or cat detection from scratch (without any priors) by just using unlabeled data [<span class="CitationRef"><a href="#CR44">44</a></span>]. Their work was a large scale investigation on the feasibility of building high-level features with Deep Learning using only unlabeled (unsupervised) data, and clearly demonstrated the benefits of using Deep Learning with unsupervised data. In Google’s experimentation, they trained a 9-layered locally connected sparse autoencoder on 10 million 200 ×200 images downloaded randomly from the Internet. The model had 1 billion connections and the training time lasted for 3 days. A computational cluster of 1000 machines and 16000 cores was used to train the network with model parallelism and asynchronous SGD (Stochastic Gradient Descent). In their experiments they obtained neurons that function like face detectors, cat detectors, and human body detectors, and based on these features their approach also outperformed the state-of-the-art and recognized 22,000 object categories from the ImageNet dataset. This demonstrates the generalization ability of abstract representations extracted by Deep Learning algorithms on new/unseen data, i.e., using features extracted from a given dataset to successfully perform a discriminative task on another dataset. While Google’s work involved the question of whether it is possible to build a face feature detector by just using unlabeled data, typically in computer vision labeled images are used to learn useful features [<span class="CitationRef"><a href="#CR45">45</a></span>]. For example, a large collection of face images with a bounding box around the faces can be used to learn a face detector feature. However, traditionally it would require a very large amount of labeled data to find the best features. The scarcity of labeled data in image data collections poses a challengingproblem.</p><p class="Para">There are other Deep Learning works that have explored image tagging. Socher et al. [<span class="CitationRef"><a href="#CR46">46</a></span>] introduce recursive neural networks for predicting a tree structure for images in multiple modalities, and is the first Deep Learning method that achieves very good results on segmentation and annotation of complex image scenes. The recursive neural network architecture is able to predict hierarchical tree structures for scene images, and outperforms other methods based on conditional random fields or a combination of other methods, as well as outperforming other existing methods in segmentation, annotation and scene classification. Socher et al. [<span class="CitationRef"><a href="#CR46">46</a></span>] also show that their algorithm is a natural tool for predicting tree structures by using it to parse natural language sentences. This demonstrates the advantage of Deep Learning as an effective approach for extracting data representations from different varieties of data types. Kumar et al. [<span class="CitationRef"><a href="#CR47">47</a></span>] suggest that recurrent neural networks can be used to construct a meaningful search space via Deep Learning, where the search space can then be used for a designed-based search.</p><p class="Para">Le et al. [<span class="CitationRef"><a href="#CR48">48</a></span>] demonstrate that Deep Learning can be used for action scene recognition as well as video data tagging, by using an independent variant analysis to learn invariant spatio-temporal features from video data. Their approach outperforms other existing methods when combined with Deep Learning techniques such as stacking and convolution to learn hierarchical representations. Previous works used to adapt hand designed feature for images like SIFT and HOG to the video domain. The Le et al. [<span class="CitationRef"><a href="#CR48">48</a></span>] study shows that extracting features directly from video data is a very important research direction, which can be also generalized to many domains.</p><p class="Para">Deep Learning has achieved remarkable results in extracting useful features (i.e., representations) for performing discriminative tasks on image and video data, as well as extracting representations from other kinds of data. These discriminative results with Deep Learning are useful for data tagging and information retrieval and can be used in search engines. Thus, the high-level complex data representations obtained by Deep Learning are useful for the application of computationally feasible and relatively simpler linear models for Big Data Analytics. However, there is considerable work that remains for further exploration, including determining appropriate objectives in learning good representations for performing discriminative tasks in Big DataAnalytics [<span class="CitationRef"><a href="#CR5">5</a></span>],[<span class="CitationRef"><a href="#CR25">25</a></span>].</p></section></div></section><section id="Sec7" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Deep learning challenges in big data analytics</h2><div class="content"><p class="Para">The prior section focused on emphasizing the applicability and benefits of Deep Learning algorithms for Big Data Analytics. However, certain characteristics associated with Big Data pose challenges for modifying and adapting Deep Learning to address those issues. This section presents some areas of Big Data where Deep Learning needs further exploration, specifically, learning with streaming data, dealing with high-dimensional data, scalability of models, and distributed computing.</p><section id="Sec8" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Incremental learning for non-stationary data</h3><p class="Para">One of the challenging aspects in Big Data Analytics is dealing with streaming and fast-moving input data. Such data analysis is useful in monitoring tasks, such as fraud detection. It is important to adapt Deep Learning to handle streaming data, as there is a need for algorithms that can deal with large amounts of continuous input data. In this section, we discuss some works associated with Deep Learning and streaming data, including incremental feature learning and extraction [<span class="CitationRef"><a href="#CR49">49</a></span>], denoising autoencoders [<span class="CitationRef"><a href="#CR50">50</a></span>], and deep belief networks [<span class="CitationRef"><a href="#CR51">51</a></span>].</p><p class="Para">Zhou et al. [<span class="CitationRef"><a href="#CR49">49</a></span>] describe how a Deep Learning algorithm can be used for incremental feature learning on very large datasets, employing denoising autoencoders [<span class="CitationRef"><a href="#CR50">50</a></span>]. Denoising autoencoders are a variant of autoencoders which extract features from corrupted input, where the extracted features are robust to noisy data and good for classification purposes. Deep Learning algorithms in general use hidden layers to contribute towards the extraction of features or data representations. In a denoising autoencoder, there is one hidden layer which extracts features, with the number of nodes in this hidden layer initially being the same as the number of features that would be extracted. Incrementally, the samples that do not conform to the given objective function (for example, their classification error is more than a threshold, or their reconstruction error is high) are collected and are used for adding new nodes to the hidden layer, with these new nodes being initialized based on those samples. Subsequently, incoming new data samples are used to jointly retrain all the features. This incremental feature learning and mapping can improve the discriminative or generative objective function; however, monotonically adding features can lead to having a lot of redundant features and overfitting of data. Consequently, similar features are merged to produce a more compact set of features. Zhou et al. [<span class="CitationRef"><a href="#CR49">49</a></span>] demonstrate that the incremental feature learning method quickly converges to the optimal number of features in a large-scale online setting. This kind of incremental feature extraction is useful in applications where the distribution of data changes with respect to time in massive online data streams. Incremental feature learning and extraction can be generalized for other Deep Learning algorithms, such as RBM [<span class="CitationRef"><a href="#CR7">7</a></span>], and makes it possible to adapt to new incoming stream of an online large-scale data. Moreover, it avoids expensive cross-validation analysis in selecting the number of features in large-scale datasets.</p><p class="Para">Calandra et al. [<span class="CitationRef"><a href="#CR51">51</a></span>] introduce adaptive deep belief networks which demonstrates how Deep Learning can be generalized to learn from online non-stationary and streaming data. Their study exploits the generative property of deep belief networks to mimic the samples from the original data, where these samples and the new observed samples are used to learn the new deep belief network which has adapted to the newly observed data. However, a downside of an adaptive deep belief network is the requirement for constant memory consumption.</p><p class="Para">The targeted works presented in this section provide empirical support to further explore and develop novel Deep Learning algorithms and architectures for analyzing large-scale, fast moving streaming data, as is encountered in some Big Data application domains such as social media feeds, marketing and financial data feeds, web click stream data, operational logs, and metering data. For example, Amazon Kinesis is a managed service designed to handle real-time streaming of Big Data – though it is not based on the Deep Learning approach.</p></section><section id="Sec9" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">High-dimensional data</h3><p class="Para">Some Deep Learning algorithms can become prohibitively computationally-expensive when dealing with high-dimensional data, such as images, likely due to the often slow learning process associated with a deep layered hierarchy of learning data abstractions and representations from a lower-level layer to a higher-level layer. That is to say, these Deep Learning algorithms can be stymied when working with Big Data that exhibits large Volume, one of the four Vs associated with Big Data Analytics. A high-dimensional data source contributes heavily to the volume of the raw data, in addition to complicating learning from the data.</p><p class="Para">Chen et al. [<span class="CitationRef"><a href="#CR52">52</a></span>] introduce marginalized stacked denoising autoencoders (mSDAs) which scale effectively for high-dimensional data and is computationally faster than regular stacked denoising autoencoders (SDAs). Their approach marginalizes noise in SDA training and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters. The marginalized denoising autoencoder layers to have hidden nodes, thus allowing a closed-form solution with substantial speed-ups. Moreover, marginalized SDA only has two free meta-parameters, controlling the amount of noise as well as the number of layers to be stacked, which greatly simplifies the model selection process. The fast training time, the capability to scale to large-scale and high-dimensional data, and implementation simplicity make mSDA a promising method with appeal to a large audience in data mining and machine learning.</p><p class="Para">Convolutional neural networks are another method which scales up effectively on high-dimensional data. Researchers have taken advantages of convolutional neural networks on ImageNet dataset with 256 ×256 RGB images to achieve state of the art results [<span class="CitationRef"><a href="#CR17">17</a></span>],[<span class="CitationRef"><a href="#CR26">26</a></span>]. In convolutional neural networks, the neurons in the hidden layers units do not need to be connected to all of the nodes in the previous layer, but just to the neurons that are in the same spatial area. Moreover, the resolution of the image data is also reduced when moving toward higher layers in the network.</p><p class="Para">The application of Deep Learning algorithms for Big Data Analytics involving high-dimensional data remains largely unexplored, and warrants development of Deep Learning based solutions that either adapt approaches similar to the ones presented above or develop novel solutions for addressing the high-dimensionality found in some Big Data domains.</p></section><section id="Sec10" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading">Large-scale models</h3><p class="Para">From a computation and analytics point of view, how do we scale the recent successes of Deep Learning to much larger-scale models and massive datasets? Empirical results have demonstrated the effectiveness of large-scale models [<span class="CitationRef"><a href="#CR53">53</a></span>]-[<span class="CitationRef"><a href="#CR55">55</a></span>], with particular focus on models with a very large number of model parameters which are able to extract more complicated features and representations [<span class="CitationRef"><a href="#CR38">38</a></span>],[<span class="CitationRef"><a href="#CR56">56</a></span>].</p><p class="Para">Dean et al. [<span class="CitationRef"><a href="#CR38">38</a></span>] consider the problem of training a Deep Learning neural network with billions of parameters using tens of thousands of CPU cores, in the context of speech recognition and computer vision. A software framework, DistBelief, is developed that can utilize computing clusters with thousands of machines to train large-scale models. The framework supports model parallelism both within a machine (via multithreading) and across machines (via message passing), with the details of parallelism, synchronization, and communication managed by DistBelief. In addition, the framework also supports data parallelism, where multiple replicas of a model are used to optimize a single objective. In order to make large-scale distributed training possible an asynchronous SGD as well as a distributed batch optimization procedure is developed that includes a distributed implementation of L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno, a quasi-Newton method for unconstrained optimization). The primary idea is to train multiple versions of the model in parallel, each running on a different node in the network and analyzing different subsets of data. The authors report that in addition to accelerating the training of conventional sized models, their framework can also train models that are larger than could be contemplated otherwise. Moreover, while the framework focuses on training large-scale neural networks, the underlying algorithms are applicable to other gradient-based learning techniques. It should be noted, however, that the extensive computational resources utilized by DistBelief are generally unavailable to a larger audience.</p><p class="Para">Coates et al. [<span class="CitationRef"><a href="#CR56">56</a></span>] leverage the relatively inexpensive computing power of a cluster of GPU servers. More specifically, they develop their own system (using neural networks) based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology and introduce a high-speed communication infrastructure to coordinate distributed computations. The system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and it can scale to networks with over 11 billion parameters using just 16 machines and where the scalability is comparable to that of DistBelief. In comparison to the computational resources used by DistBelief, the distributed system network based on COTS HPC is more generally available to a larger audience, making it a reasonable alternative for other Deep Learning experts exploring large-scale models.</p><p class="Para">Large-scale Deep Learning models are quite suited to handle massive volumes of input associated with Big Data, and as demonstrated in the above works they are also better at learning complex data patterns from large volumes of data. Determining the optimal number of model parameters in such large-scale models and improving their computational practicality pose challenges in Deep Learning for Big Data Analytics. In addition to the problem of handling massive volumes of data, large-scale Deep Learning models for Big Data Analytics also have to contend with other Big Data problems, such as domain adaptation (see next section) and streaming data. This lends to the need for further innovations in large-scale models for Deep Learning algorithms and architectures.</p></section></div></section><section id="Sec11" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Future work on deep learning in big data analytics</h2><div class="content"><p class="Para">In the prior sections, we discussed some recent applications of Deep Learning algorithms for Big Data Analytics, as well as identified some areas where Deep Learning research needs further exploration to address specific data analysis problems observed in Big Data. Considering the low-maturity of Deep Learning, we note that considerable work remains to done. In this section, we discuss our insights on some remaining questions in Deep Learning research, especially on work needed for improving machine learning and the formulation of the high-level abstractions and data representations for Big Data.</p><p class="Para">An important problem is whether to utilize the entire Big Data input corpus available when analyzing data with Deep Learning algorithms. The general focus is to apply Deep Learning algorithms to train the high-level data representation patterns based on a portion of the available input corpus, and then utilize the remaining input corpus with the learnt patterns for extracting the data abstractions and representations. In the context of this problem, a question to explore is what volume of input data is generally necessary to train useful (good) data representations by Deep Learning algorithms which can then be generalized for new data in the specific Big Data application domain.</p><p class="Para">Upon further exploring the above problem, we recall the Variety characteristic of Big Data Analytics, which focuses on the variation of the input data types and domains in Big Data. here, by considering the shift between the input data source (for training the representations) and the target data source (for generalizing the representations), the problem becomes one of domain adaptation for Deep Learning in Big Data Analytics. Domain adaptation during learning is an important focus of study in Deep Learning [<span class="CitationRef"><a href="#CR57">57</a></span>],[<span class="CitationRef"><a href="#CR58">58</a></span>], where the distribution of the training data (from which the representations are learnt) is different from the distribution of the test data (on which the learnt representations are deployed).</p><p class="Para">Glorot et al. [<span class="CitationRef"><a href="#CR57">57</a></span>] demonstrate that Deep Learning is able to discover intermediate data representations in a hierarchical learning manner, and that these representations are meaningful to, and can be shared among, different domains. In their work, a stacked denoising autoencoder is initially used to learn features and patterns from unlabeled data obtained from different source domains. Subsequently, a support vector machine (SVM) algorithm utilizes the learnt features and patterns for application on labeled data from a given source domain, resulting in a linear classification model that outperforms other methods. This domain adaptation study is successfully applied on a large industrial strength dataset consisting of 22 source domains. However, it should be noted that their study does not explicitly encode the distribution shift of the data between the source domain and the target domains. Chopra et al. [<span class="CitationRef"><a href="#CR58">58</a></span>] propose a Deep Learning model (based on neural networks) for domain adaptation which strives to learn a useful (for prediction purposes) representation of the unsupervised data by taking into consideration information available from the distribution shift between the training and test data. The focus is to hierarchically learn multiple intermediate representations along an interpolating path between the training and testing domains. In the context of object recognition, their study demonstrates an improvement over other methods. The two studies presented above raise the question about how to increase the generalization capacity of Deep Learning data representations and patterns, noting that the ability to generalize learnt patterns is an important requirement in Big Data Analytics where often there is a distribution shift between the input domain and the target domain.</p><p class="Para">Another key area of interest would be to explore the question of what criteria is necessary and should be defined for allowing the extracted data representations to provide useful semantic meaning to the Big Data. Earlier, we discussed some studies that utilize the data representations extracted through Deep Learning for semantic indexing. Bengio et al. [<span class="CitationRef"><a href="#CR5">5</a></span>] present some characteristics of what constitutes good data representations for performing discriminative tasks, and point to the open question regarding the definition of the criteria for learning good data representations in Deep Learning. Compared to more conventional learning algorithms where misclassification error is generally used as an important criterion for model training and learning patterns, defining a corresponding criteria for training Deep Learning algorithms with Big Data is unsuitable since most Big Data Analytics involve learning from largely unsupervised data. While availability of supervised data in some Big Data domains can be helpful, the question of defining the criteria for obtaining good data abstractions and representations still remains largely unexplored in Big Data Analytics. Moreover, the question of defining the criteria required for extracting good data representations leads to the question of what would constitute a good data representation that is effective for semantic indexing and/or data tagging.</p><p class="Para">In some Big Data domains, the input corpus consists of a mix of both labeled and unlabeled data, e.g., cyber security [<span class="CitationRef"><a href="#CR59">59</a></span>], fraud detection [<span class="CitationRef"><a href="#CR60">60</a></span>], and computer vision [<span class="CitationRef"><a href="#CR45">45</a></span>]. In such cases, Deep Learning algorithms can incorporate semi-supervised training methods towards the goal of defining criteria for good data representation learning. For example, following learning representations and patterns from the unlabeled/unsupervised data, the available labeled/supervised data can be exploited to further tune and improve the learnt representations and patterns for a specific analytics task, including semantic indexing or discriminative modeling. A variation of semi-supervised learning in data mining, active learning methods could also be applicable towards obtaining improved data representations where input from crowdsourcing or human experts can be used to obtain labels for some data samples which can then be used to better tune and improve the learnt data representations.</p></div></section><section id="Sec12" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Conclusion</h2><div class="content"><p class="Para">In contrast to more conventional machine learning and feature engineering algorithms, Deep Learning has an advantage of potentially providing a solution to address the data analysis and learning problems found in massive volumes of input data. More specifically, it aids in automatically extracting complex data representations from large volumes of unsupervised data. This makes it a valuable tool for Big Data Analytics, which involves data analysis from very large collections of raw data that is generally unsupervised and un-categorized. The hierarchical learning and extraction of different levels of complex, data abstractions in Deep Learning provides a certain degree of simplification for Big Data Analytics tasks, especially for analyzing massive volumes of data, semantic indexing, data tagging, information retrieval, and discriminative tasks such a classification and prediction.</p><p class="Para">In the context of discussing key works in the literature and providing our insights on those specific topics, this study focused on two important areas related to Deep Learning and Big Data: (1) the application of Deep Learning algorithms and architectures for Big Data Analytics, and (2) how certain characteristics and issues of Big Data Analytics pose unique challenges towards adapting Deep Learning algorithms for those problems. A targeted survey of important literature in Deep Learning research and application to different domains is presented in the paper as a means to identify how Deep Learning can be used for different purposes in Big Data Analytics.</p><p class="Para">The low-maturity of the Deep Learning field warrants extensive further research. In particular, more work is necessary on how we can adapt Deep Learning algorithms for problems associated with Big Data, including high dimensionality, streaming data analysis, scalability of Deep Learning models, improved formulation of data abstractions, distributed computing, semantic indexing, data tagging, information retrieval, criteria for extracting good data representations, and domain adaptation. Future works should focus on addressing one or more of these problems often seen in Big Data, thus contributing to the Deep Learning and Big Data Analytics research corpus.</p></div></section></div><section id="Notes" class="Section1 RenderAsSection1"><h2 class="Heading">Notes</h2><div class="content"><aside class="ArticleNote ArticleNoteMisc"><h2 class="Heading">Competing interests</h2><p class="SimplePara">The authors declare that they have no competing interests.</p></aside><aside class="ArticleNote ArticleNoteMisc"><h2 class="Heading">Authors’ contributions</h2><p class="SimplePara">MMN performed the primary literature review and analysis for this work, and also drafted the manuscript. RW and NS worked with MMN to develop the article’s framework and focus. TMK, FV and EM introduced this topic to MMN and TMK coordinated with the other authors to complete and finalize this work. All authors read and approved the final manuscript.</p></aside></div></section><section class="Section1 RenderAsSection1" id="Bib1" tabindex="-1"><h2 class="Heading">References</h2><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Domingos P (2012) A few useful things to know about machine learning. Commun ACM 55(10)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20few%20useful%20things%20to%20know%20about%20machine%20learning&amp;author=P.%20Domingos&amp;author=P.%20Domingos&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference On. IEEE Vol. 1. pp 886–893<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Histograms%20of%20oriented%20gradients%20for%20human%20detection&amp;author=N.%20Dalal&amp;author=B.%20Triggs&amp;author=N.%20Dalal&amp;author=B.%20Triggs&amp;publication_year=2005"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Lowe DG (1999) Object recognition from local scale-invariant features. In: Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference On. IEEE Computer Society Vol. 2. pp 1150–1157<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Object%20recognition%20from%20local%20scale-invariant%20features.&amp;author=DG.%20Lowe&amp;publication_year=1999%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Bengio Y, LeCun Y: <strong class="EmphasisTypeBold ">Scaling learning algorithms towards, AI.</strong>In <em class="EmphasisTypeItalic ">Large Scale Kernel Machines</em> Edited by: Bottou L, Chapelle O, DeCoste D, Weston J. MIT Press, Cambridge, MA; 2007, 321–360. [<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf"><span class="RefSource">http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf</span></a></span>] http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Scaling%20learning%20algorithms%20towards%2C%20AI&amp;author=Y.%20Bengio&amp;author=Y.%20LeCun&amp;pages=321-360&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Bengio Y, Courville A, Vincent P: <strong class="EmphasisTypeBold ">Representation learning: A review and new perspectives.</strong> <em class="EmphasisTypeItalic ">Pattern Analysis and Machine Intelligence, IEEE Transactions on</em> 2013,<strong class="EmphasisTypeBold ">35</strong>(8):1798–1828. doi:10.1109/TPAMI.2013.50 doi:10.1109/TPAMI.2013.50 10.1109/TPAMI.2013.50<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TPAMI.2013.50"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Representation%20learning%3A%20A%20review%20and%20new%20perspectives&amp;author=Y.%20Bengio&amp;author=A.%20Courville&amp;author=P.%20Vincent&amp;journal=Pattern%20Analysis%20and%20Machine%20Intelligence%2C%20IEEE%20Transactions%20on&amp;volume=35&amp;issue=8&amp;pages=1798-1828&amp;publication_year=2013&amp;doi=10.1109%2FTPAMI.2013.50"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Arel I, Rose DC, Karnowski TP: <strong class="EmphasisTypeBold ">Deep machine learning-a new frontier in artificial intelligence research [research frontier].</strong> <em class="EmphasisTypeItalic ">IEEE Comput Intell</em> 2010, <strong class="EmphasisTypeBold ">5:</strong> 13–18. 10.1109/MCI.2010.938364<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/MCI.2010.938364"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20machine%20learning-a%20new%20frontier%20in%20artificial%20intelligence%20research%20%5Bresearch%20frontier%5D&amp;author=I.%20Arel&amp;author=DC.%20Rose&amp;author=TP.%20Karnowski&amp;journal=IEEE%20Comput%20Intell&amp;volume=5&amp;pages=13-18&amp;publication_year=2010&amp;doi=10.1109%2FMCI.2010.938364"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Hinton GE, Osindero S, Teh Y-W: <strong class="EmphasisTypeBold ">A fast learning algorithm for deep belief nets.</strong> <em class="EmphasisTypeItalic ">Neural Comput</em> 2006,<strong class="EmphasisTypeBold ">18</strong>(7):1527–1554. 10.1162/neco.2006.18.7.1527<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1106.68094"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2224485"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1162/neco.2006.18.7.1527"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets&amp;author=GE.%20Hinton&amp;author=S.%20Osindero&amp;author=Y-W.%20Teh&amp;journal=Neural%20Comput&amp;volume=18&amp;issue=7&amp;pages=1527-1554&amp;publication_year=2006&amp;doi=10.1162%2Fneco.2006.18.7.1527"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Bengio Y, Lamblin P, Popovici D, Larochelle H2007. Greedy layer-wise training of deep networks, Vol. 19. Bengio Y, Lamblin P, Popovici D, Larochelle H2007. Greedy layer-wise training of deep networks, Vol. 19.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bengio%20Y%2C%20Lamblin%20P%2C%20Popovici%20D%2C%20Larochelle%20H2007.%20Greedy%20layer-wise%20training%20of%20deep%20networks%2C%20Vol.%2019.%20Bengio%20Y%2C%20Lamblin%20P%2C%20Popovici%20D%2C%20Larochelle%20H2007.%20Greedy%20layer-wise%20training%20of%20deep%20networks%2C%20Vol.%2019."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Larochelle H, Bengio Y, Louradour J, Lamblin P: <strong class="EmphasisTypeBold ">Exploring strategies for training deep neural networks.</strong> <em class="EmphasisTypeItalic ">J Mach Learn Res</em> 2009, <strong class="EmphasisTypeBold ">10:</strong> 1–40.<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1235.68168"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Exploring%20strategies%20for%20training%20deep%20neural%20networks&amp;author=H.%20Larochelle&amp;author=Y.%20Bengio&amp;author=J.%20Louradour&amp;author=P.%20Lamblin&amp;journal=J%20Mach%20Learn%20Res&amp;volume=10&amp;pages=1-40&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Salakhutdinov R, Hinton GE (2009) Deep boltzmann machines. In: International Conference on, Artificial Intelligence and Statistics. JMLR.org. pp 448–455<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20boltzmann%20machines&amp;author=R.%20Salakhutdinov&amp;author=GE.%20Hinton&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Goodfellow I, Lee H, Le QV, Saxe A, Ng AY (2009) Measuring invariances in deep networks. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 646–654<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Measuring%20invariances%20in%20deep%20networks&amp;author=I.%20Goodfellow&amp;author=H.%20Lee&amp;author=QV.%20Le&amp;author=A.%20Saxe&amp;author=AY.%20Ng&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Dahl G, Ranzato M, Mohamed A-R, Hinton GE (2010) Phone recognition with the mean-covariance restricted boltzmann machine. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 469–477<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Phone%20recognition%20with%20the%20mean-covariance%20restricted%20boltzmann%20machine.&amp;author=G.%20Dahl&amp;author=M.%20Ranzato&amp;author=A-R.%20Mohamed&amp;author=GE.%20Hinton&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Hinton G, Deng L, Yu D, Mohamed A-R, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath T, Dahl G, Kingsbury B: <strong class="EmphasisTypeBold ">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.</strong> <em class="EmphasisTypeItalic ">Signal Process Mag IEEE</em> 2012,<strong class="EmphasisTypeBold ">29</strong>(6):82–97. 10.1109/MSP.2012.2205597<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.2012.2205597"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups&amp;author=G.%20Hinton&amp;author=L.%20Deng&amp;author=D.%20Yu&amp;author=A-R.%20Mohamed&amp;author=N.%20Jaitly&amp;author=A.%20Senior&amp;author=V.%20Vanhoucke&amp;author=P.%20Nguyen&amp;author=T.%20Sainath&amp;author=G.%20Dahl&amp;author=B.%20Kingsbury&amp;journal=Signal%20Process%20Mag%20IEEE&amp;volume=29&amp;issue=6&amp;pages=82-97&amp;publication_year=2012&amp;doi=10.1109%2FMSP.2012.2205597"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Seide F, Li G, Yu D (2011) Conversational speech transcription using context-dependent deep neural networks. In: INTERSPEECH. ISCA. pp 437–440<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Conversational%20speech%20transcription%20using%20context-dependent%20deep%20neural%20networks&amp;author=F.%20Seide&amp;author=G.%20Li&amp;author=D.%20Yu&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Mohamed A-R, Dahl GE, Hinton G: <strong class="EmphasisTypeBold ">Acoustic modeling using deep belief networks.</strong> <em class="EmphasisTypeItalic ">Audio Speech Lang Process IEEE Trans</em> 2012,<strong class="EmphasisTypeBold ">20</strong>(1):14–22. 10.1109/TASL.2011.2109382<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2011.2109382"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Acoustic%20modeling%20using%20deep%20belief%20networks&amp;author=A-R.%20Mohamed&amp;author=GE.%20Dahl&amp;author=G.%20Hinton&amp;journal=Audio%20Speech%20Lang%20Process%20IEEE%20Trans&amp;volume=20&amp;issue=1&amp;pages=14-22&amp;publication_year=2012&amp;doi=10.1109%2FTASL.2011.2109382"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Dahl GE, Yu D, Deng L, Acero A: <strong class="EmphasisTypeBold ">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.</strong> <em class="EmphasisTypeItalic ">Audio Speech Lang Process IEEE Trans</em> 2012,<strong class="EmphasisTypeBold ">20</strong>(1):30–42. 10.1109/TASL.2011.2134090<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2011.2134090"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Context-dependent%20pre-trained%20deep%20neural%20networks%20for%20large-vocabulary%20speech%20recognition&amp;author=GE.%20Dahl&amp;author=D.%20Yu&amp;author=L.%20Deng&amp;author=A.%20Acero&amp;journal=Audio%20Speech%20Lang%20Process%20IEEE%20Trans&amp;volume=20&amp;issue=1&amp;pages=30-42&amp;publication_year=2012&amp;doi=10.1109%2FTASL.2011.2134090"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Krizhevsky A, Sutskever I, Hinton G (2012) Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. Vol. 25. pp 1106–1114<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks.&amp;author=A.%20Krizhevsky&amp;author=I.%20Sutskever&amp;author=G.%20Hinton&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Mikolov T, Deoras A, Kombrink S, Burget L, Cernock`y J (2011) Empirical evaluation and combination of advanced language modeling techniques. In: INTERSPEECH. ISCA. pp 605–608<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Empirical%20evaluation%20and%20combination%20of%20advanced%20language%20modeling%20techniques&amp;author=T.%20Mikolov&amp;author=A.%20Deoras&amp;author=S.%20Kombrink&amp;author=L.%20Burget&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Socher R, Huang EH, Pennin J, Manning CD, Ng A (2011) Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 801–809<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Dynamic%20pooling%20and%20unfolding%20recursive%20autoencoders%20for%20paraphrase%20detection&amp;author=R.%20Socher&amp;author=EH.%20Huang&amp;author=J.%20Pennin&amp;author=CD.%20Manning&amp;author=A.%20Ng&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Bordes A, Glorot X, Weston J, Bengio Y (2012) Joint learning of words and meaning representations for open-text semantic parsing. In: International Conference on Artificial Intelligence and Statistics. JMLR.org. pp 127–135<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Joint%20learning%20of%20words%20and%20meaning%20representations%20for%20open-text%20semantic%20parsing&amp;author=A.%20Bordes&amp;author=X.%20Glorot&amp;author=J.%20Weston&amp;author=Y.%20Bengio&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">National Research Council: <em class="EmphasisTypeItalic ">Frontiers in Massive Data Analysis</em>. The National Academies Press, Washington, DC; 2013.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Frontiers%20in%20Massive%20Data%20Analysis&amp;author=Council.%20National%20Research&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Dumbill E: <strong class="EmphasisTypeBold ">What Is Big Data? An Introduction to the Big Data Landscape.</strong> In <em class="EmphasisTypeItalic ">Strata 2012: Making Data Work</em>. O’Reilly, Santa Clara, CA O’Reilly; 2012.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=What%20Is%20Big%20Data%3F%20An%20Introduction%20to%20the%20Big%20Data%20Landscape&amp;author=E.%20Dumbill&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Khoshgoftaar TM (2013) Overcoming big data challenges. In: Proceedings of the 25th International Conference on Software Engineering and Knowledge Engineering, Boston, MA. ICSE. Invited Keynote Speaker<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Overcoming%20big%20data%20challenges&amp;author=TM.%20Khoshgoftaar&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Bengio Y: <em class="EmphasisTypeItalic ">Learning Deep Architectures for AI</em>. Now Publishers Inc., Hanover, MA, USA; 2009.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Learning%20Deep%20Architectures%20for%20AI&amp;author=Y.%20Bengio&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Bengio Y: <strong class="EmphasisTypeBold ">Deep learning of representations: Looking forward.</strong> In <em class="EmphasisTypeItalic ">Proceedings of the 1st International Conference on Statistical Language and Speech Processing. SLSP’13</em>. Springer, Tarragona, Spain; 2013:1–37. http://dx.doi.org/10.1007/978–3-642–39593–2_1 http://dx.doi.org/10.1007/978-3-642-39593-2_1 10.1007/978-3-642-39593-2_1<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-642-39593-2_1"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20learning%20of%20representations%3A%20Looking%20forward&amp;author=Y.%20Bengio&amp;pages=1-37&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Hinton GE, Salakhutdinov RR (Science) Reducing the dimensionality of data with neural networks313(5786): 504–507. Hinton GE, Salakhutdinov RR (Science) Reducing the dimensionality of data with neural networks313(5786): 504–507.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hinton%20GE%2C%20Salakhutdinov%20RR%20%28Science%29%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks313%285786%29%3A%20504%E2%80%93507.%20Hinton%20GE%2C%20Salakhutdinov%20RR%20%28Science%29%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks313%285786%29%3A%20504%E2%80%93507."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Hinton GE, Zemel RS: <strong class="EmphasisTypeBold ">Autoencoders, minimum description length, and helmholtz free energy.</strong> <em class="EmphasisTypeItalic ">Adv Neural Inform Process Syst</em> 1994, <strong class="EmphasisTypeBold ">6:</strong> 3–10.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Autoencoders%2C%20minimum%20description%20length%2C%20and%20helmholtz%20free%20energy&amp;author=GE.%20Hinton&amp;author=RS.%20Zemel&amp;journal=Adv%20Neural%20Inform%20Process%20Syst&amp;volume=6&amp;pages=3-10&amp;publication_year=1994"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Smolensky P (1986) Information processing in dynamical systems: foundations of harmony theory. In: Parallel Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press. Vol. 1. pp 194–281<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Information%20processing%20in%20dynamical%20systems%3A%20foundations%20of%20harmony%20theory&amp;author=P.%20Smolensky&amp;author=P.%20Smolensky&amp;publication_year=1986"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Hinton GE: <strong class="EmphasisTypeBold ">Training products of experts by minimizing contrastive divergence.</strong> <em class="EmphasisTypeItalic ">Neural Comput</em> 2002,<strong class="EmphasisTypeBold ">14</strong>(8):1771–1800. 10.1162/089976602760128018<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1010.68111"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2978160"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1162/089976602760128018"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Training%20products%20of%20experts%20by%20minimizing%20contrastive%20divergence&amp;author=GE.%20Hinton&amp;journal=Neural%20Comput&amp;volume=14&amp;issue=8&amp;pages=1771-1800&amp;publication_year=2002&amp;doi=10.1162%2F089976602760128018"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Garshol LM (2013) Introduction to Big Data/Machine Learning. Online Slide Show, . http://www.slideshare.net/larsga/introduction-to-big-datamachine-learning., [<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.slideshare.net/larsga/introduction-to-big-datamachine-learning"><span class="RefSource">http://www.slideshare.net/larsga/introduction-to-big-datamachine-learning</span></a></span>]<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Introduction%20to%20Big%20Data%2FMachine%20Learning&amp;author=LM.%20Garshol&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Grobelnik M (2013) Big Data Tutorial. European Data Forum. http://www.slideshare.net/EUDataForum/edf2013-big-datatutorialmarkogrobelnik?related=1., Grobelnik M (2013) Big Data Tutorial. European Data Forum. . <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.slideshare.net/EUDataForum/edf2013-big-datatutorialmarkogrobelnik?related=1"><span class="RefSource">http://www.slideshare.net/EUDataForum/edf2013-big-datatutorialmarkogrobelnik?related=1</span></a></span> Grobelnik M (2013) Big Data Tutorial. European Data Forum. .<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Big%20Data%20Tutorial.&amp;author=M.%20Grobelnik&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Salton G, Buckley C: <strong class="EmphasisTypeBold ">Term-weighting approaches in automatic text retrieval.</strong> <em class="EmphasisTypeItalic ">Inform Process Manag</em> 1988,<strong class="EmphasisTypeBold ">24</strong>(5):513–523. 10.1016/0306-4573(88)90021-0<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/0306-4573(88)90021-0"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Term-weighting%20approaches%20in%20automatic%20text%20retrieval&amp;author=G.%20Salton&amp;author=C.%20Buckley&amp;journal=Inform%20Process%20Manag&amp;volume=24&amp;issue=5&amp;pages=513-523&amp;publication_year=1988&amp;doi=10.1016%2F0306-4573%2888%2990021-0"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Robertson SE, Walker S (1994) Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In: Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. pp 232–241. Springer-Verlag New York, Inc<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Some%20simple%20effective%20approximations%20to%20the%202-poisson%20model%20for%20probabilistic%20weighted%20retrieval&amp;author=SE.%20Robertson&amp;author=S.%20Walker&amp;publication_year=1994"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Hinton G, Salakhutdinov R: <strong class="EmphasisTypeBold ">Discovering binary codes for documents by learning deep generative models.</strong> <em class="EmphasisTypeItalic ">Topics Cogn Sci</em> 2011,<strong class="EmphasisTypeBold ">3</strong>(1):74–91. 10.1111/j.1756-8765.2010.01109.x<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1111/j.1756-8765.2010.01109.x"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Discovering%20binary%20codes%20for%20documents%20by%20learning%20deep%20generative%20models&amp;author=G.%20Hinton&amp;author=R.%20Salakhutdinov&amp;journal=Topics%20Cogn%20Sci&amp;volume=3&amp;issue=1&amp;pages=74-91&amp;publication_year=2011&amp;doi=10.1111%2Fj.1756-8765.2010.01109.x"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Salakhutdinov R, Hinton G: <strong class="EmphasisTypeBold ">Semantic hashing.</strong> <em class="EmphasisTypeItalic ">Int J Approximate, Reasoning</em> 2009,<strong class="EmphasisTypeBold ">50</strong>(7):969–978. 10.1016/j.ijar.2008.11.006<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.ijar.2008.11.006"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Semantic%20hashing&amp;author=R.%20Salakhutdinov&amp;author=G.%20Hinton&amp;journal=Int%20J%20Approximate%2C%20Reasoning&amp;volume=50&amp;issue=7&amp;pages=969-978&amp;publication_year=2009&amp;doi=10.1016%2Fj.ijar.2008.11.006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Ranzato M, Szummer M (2008) Semi-supervised learning of compact document representations with deep=networks. In: Proceedings of the 25th International Conference on Machine Learning. ACM. pp 792–799<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Semi-supervised%20learning%20of%20compact%20document%20representations%20with%20deep%3Dnetworks&amp;author=M.%20Ranzato&amp;author=M.%20Szummer&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Mikolov T, Chen K, Dean J (2013) Efficient estimation of word representations in vector space. CoRR: Computing Research Repository: 1–12. abs/1301.3781<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Efficient%20estimation%20of%20word%20representations%20in%20vector%20space&amp;author=T.%20Mikolov&amp;author=K.%20Chen&amp;author=J.%20Dean&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Dean J, Corrado G, Monga R, Chen K, Devin M, Le Q, Mao M, Ranzato M, Senior A, Tucker P, Yang K, Ng A (2012) Large scale distributed deep networks. In: Bartlett P, Pereira FCN, Burges CJC, Bottou L, Weinberger KQ (eds)Advances in Neural Information Processing Systems, 1232–1240. http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf., Dean J, Corrado G, Monga R, Chen K, Devin M, Le Q, Mao M, Ranzato M, Senior A, Tucker P, Yang K, Ng A (2012) Large scale distributed deep networks. In: Bartlett P, Pereira FCN, Burges CJC, Bottou L, Weinberger KQ (eds)Advances in Neural Information Processing Systems, 1232–1240. . <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf"><span class="RefSource">http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Large%20scale%20distributed%20deep%20networks.&amp;author=J.%20Dean&amp;author=G.%20Corrado&amp;author=R.%20Monga&amp;author=K.%20Chen&amp;author=M.%20Devin&amp;author=Q.%20Le&amp;author=M.%20Mao&amp;author=M.%20Ranzato&amp;author=A.%20Senior&amp;author=P.%20Tucker&amp;author=K.%20Yang&amp;author=A.%20Ng&amp;pages=1232-1240&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Mikolov T, Le QV, Sutskever I (2013) Exploiting similarities among languages for machine translation. CoRR: Comput Res Repository: 1–10. abs/1309.4168<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Exploiting%20similarities%20among%20languages%20for%20machine%20translation&amp;author=T.%20Mikolov&amp;author=QV.%20Le&amp;author=I.%20Sutskever&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Li G, Zhu H, Cheng G, Thambiratnam K, Chitsaz B, Yu D, Seide F (2012) Context-dependent deep neural networks for audio indexing of real-life data. In: Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE. pp 143–148<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Context-dependent%20deep%20neural%20networks%20for%20audio%20indexing%20of%20real-life%20data&amp;author=G.%20Li&amp;author=H.%20Zhu&amp;author=G.%20Cheng&amp;author=K.%20Thambiratnam&amp;author=B.%20Chitsaz&amp;author=D.%20Yu&amp;author=F.%20Seide&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Zipern A (2001) A Quick Way to Search For Images on the Web. The New York Times. News Watch Article. http://www.nytimes.com/2001/07/12/technology/news-watch-a-quick-way-to-search-for-images-on-the-web.html., Zipern A (2001) A Quick Way to Search For Images on the Web. The New York Times. News Watch Article. . <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.nytimes.com/2001/07/12/technology/news-watch-a-quick-way-to-search-for-images-on-the-web.html"><span class="RefSource">http://www.nytimes.com/2001/07/12/technology/news-watch-a-quick-way-to-search-for-images-on-the-web.html</span></a></span><span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20Quick%20Way%20to%20Search%20For%20Images%20on%20the%20Web&amp;author=A.%20Zipern&amp;publication_year=2001"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Cusumano MA: <strong class="EmphasisTypeBold ">Google: What it is and what it is not.</strong> <em class="EmphasisTypeItalic ">Commun ACM - Med Image Moeling</em> 2005,<strong class="EmphasisTypeBold ">48</strong>(2):15–17. doi:10.1145/1042091.1042107 doi:10.1145/1042091.1042107 10.1145/1042091.1042107<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1145/1042091.1042107"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Google%3A%20What%20it%20is%20and%20what%20it%20is%20not&amp;author=MA.%20Cusumano&amp;journal=Commun%20ACM%20-%20Med%20Image%20Moeling&amp;volume=48&amp;issue=2&amp;pages=15-17&amp;publication_year=2005&amp;doi=10.1145%2F1042091.1042107"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Lee H, Battle A, Raina R, Ng A (2006) Efficient sparse coding algorithms. In: Advances in Neural Information Processing Systems. MIT Press. pp 801–808<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Efficient%20sparse%20coding%20algorithms&amp;author=H.%20Lee&amp;author=A.%20Battle&amp;author=R.%20Raina&amp;author=A.%20Ng&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Le Q, Ranzato M, Monga R, Devin M, Chen K, Corrado G, Dean J, Ng A (2012) Building high-level features using large scale unsupervised learning. In: Proceeding of the 29th International Conference in Machine Learning, Edingburgh, Scotland<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning&amp;author=Q.%20Le&amp;author=M.%20Ranzato&amp;author=R.%20Monga&amp;author=M.%20Devin&amp;author=K.%20Chen&amp;author=G.%20Corrado&amp;author=J.%20Dean&amp;author=A.%20Ng&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Freytag A, Rodner E, Bodesheim P, Denzler J: <strong class="EmphasisTypeBold ">Labeling Examples that Matter: Relevance-Based Active Learning with Gaussian Processes.</strong> In <em class="EmphasisTypeItalic ">35th German Conference on Pattern Recognition (GCPR)</em>. Saarland University and Max-Planck-Institute for Informatics, Germany; 2013:282–291.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Labeling%20Examples%20that%20Matter%3A%20Relevance-Based%20Active%20Learning%20with%20Gaussian%20Processes&amp;author=A.%20Freytag&amp;author=E.%20Rodner&amp;author=P.%20Bodesheim&amp;author=J.%20Denzler&amp;pages=282-291&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Socher R, Lin CC, Ng A, Manning C (2011) Parsing natural scenes and natural language with recursive neural networks. In: Proceedings of the 28th International Conference on Machine Learning. Omnipress. pp 129–136<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks&amp;author=R.%20Socher&amp;author=CC.%20Lin&amp;author=A.%20Ng&amp;author=C.%20Manning&amp;pages=129-136&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Kumar R, Talton JO, Ahmad S, Klemmer SR (2012) Data-driven web design. In: Proceedings of the 29th International Conference on Machine Learning. icml.cc/Omnipress<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Data-driven%20web%20design&amp;author=R.%20Kumar&amp;author=JO.%20Talton&amp;author=S.%20Ahmad&amp;author=SR.%20Klemmer&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Le QV, Zou WY, Yeung SY, Ng AY (2011) Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In: Computer Vision and Pattern Recognition (CVPR) 2011 IEEE Conference On. IEEE. pp 3361–3368<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Learning%20hierarchical%20invariant%20spatio-temporal%20features%20for%20action%20recognition%20with%20independent%20subspace%20analysis&amp;author=QV.%20Le&amp;author=WY.%20Zou&amp;author=SY.%20Yeung&amp;author=AY.%20Ng&amp;pages=3361-3368&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Zhou G, Sohn K, Lee H (2012) Online incremental feature learning with denoising autoencoders. In: International Conference on Artificial Intelligence and Statistics. JMLR.org. pp 1453–1461<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Online%20incremental%20feature%20learning%20with%20denoising%20autoencoders&amp;author=G.%20Zhou&amp;author=K.%20Sohn&amp;author=H.%20Lee&amp;pages=1453-1461&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Vincent P, Larochelle H, Bengio Y, Manzagol P-A (2008) Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th International Conference on Machine Learning. ACM. pp 1096–1103<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders&amp;author=P.%20Vincent&amp;author=H.%20Larochelle&amp;author=Y.%20Bengio&amp;author=P-A.%20Manzagol&amp;pages=1096-1103&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Calandra R, Raiko T, Deisenroth MP, Pouzols FM: <strong class="EmphasisTypeBold ">Learning deep belief networks from non-stationary streams.</strong> In <em class="EmphasisTypeItalic ">Artificial Neural Networks and Machine Learning–ICANN 2012</em>. Springer, Berlin Heidelberg; 2012:379–386. 10.1007/978-3-642-33266-1_47<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-642-33266-1_47"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Learning%20deep%20belief%20networks%20from%20non-stationary%20streams&amp;author=R.%20Calandra&amp;author=T.%20Raiko&amp;author=MP.%20Deisenroth&amp;author=FM.%20Pouzols&amp;pages=379-386&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. In: Proceeding of the 29th International Conference in Machine Learning, Edingburgh, Scotland<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Marginalized%20denoising%20autoencoders%20for%20domain%20adaptation&amp;author=M.%20Chen&amp;author=ZE.%20Xu&amp;author=KQ.%20Weinberger&amp;author=F.%20Sha&amp;author=M.%20Chen&amp;author=ZE.%20Xu&amp;author=KQ.%20Weinberger&amp;author=F.%20Sha&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Coates A, Ng A (2011) The importance of encoding versus training with sparse coding and vector quantization. In: Proceedings of the 28th International Conference on Machine Learning. Omnipress. pp 921–928<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=The%20importance%20of%20encoding%20versus%20training%20with%20sparse%20coding%20and%20vector%20quantization&amp;author=A.%20Coates&amp;author=A.%20Ng&amp;pages=921-928&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov R (2012) Improving neural networks by preventing co-adaptation of feature detectors. CoRR: Comput Res Repository: 1–18. abs/1207.0580<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Improving%20neural%20networks%20by%20preventing%20co-adaptation%20of%20feature%20detectors&amp;author=GE.%20Hinton&amp;author=N.%20Srivastava&amp;author=A.%20Krizhevsky&amp;author=I.%20Sutskever&amp;author=R.%20Salakhutdinov&amp;pages=1-18&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Goodfellow IJ, Warde-Farley D, Mirza M, Courville A, Bengio Y (2013) Maxout networks. In: Proceeding of the 30th International Conference in Machine Learning, Atlanta, GA<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Maxout%20networks.&amp;author=IJ.%20Goodfellow&amp;author=D.%20Warde-Farley&amp;author=M.%20Mirza&amp;author=A.%20Courville&amp;author=Y.%20Bengio&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Coates A, Huval B, Wang T, Wu D, Catanzaro B, Andrew N (2013) Deep learning with cots hpc systems. In: Proceedings of the 30th International Conference on Machine Learning. pp 1337–1345<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Deep%20learning%20with%20cots%20hpc%20systems.&amp;author=A.%20Coates&amp;author=B.%20Huval&amp;author=T.%20Wang&amp;author=D.%20Wu&amp;author=B.%20Catanzaro&amp;author=N.%20Andrew&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Glorot X, Bordes A, Bengio Y (2011) Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp 513–520<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach&amp;author=X.%20Glorot&amp;author=A.%20Bordes&amp;author=Y.%20Bengio&amp;publication_year=2011%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Chopra S, Balakrishnan S, Gopalan R (2013) Dlid: Deep learning for domain adaptation by interpolating betweendomains. In: Workshop on Challenges in Representation Learning, Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Dlid%3A%20Deep%20learning%20for%20domain%20adaptation%20by%20interpolating%20betweendomains.&amp;author=S.%20Chopra&amp;author=S.%20Balakrishnan&amp;author=R.%20Gopalan&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Suthaharan S: <strong class="EmphasisTypeBold ">Big data classification: Problems and challenges in network intrusion prediction with machine learning.</strong> In <em class="EmphasisTypeItalic ">ACM Sigmetrics: Big Data Analytics Workshop</em>. ACM, Pittsburgh, PA; 2013.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Big%20data%20classification%3A%20Problems%20and%20challenges%20in%20network%20intrusion%20prediction%20with%20machine%20learning&amp;author=S.%20Suthaharan&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Wang W, Lu D, Zhou X, Zhang B, Mu J: <strong class="EmphasisTypeBold ">Statistical wavelet-based anomaly detection in big data with compressive sensing.</strong> <em class="EmphasisTypeItalic ">EURASIP J Wireless Commun Netw</em> 2013, <strong class="EmphasisTypeBold ">2013:</strong> 269. http://www.bibsonomy.org/bibtex/25e432dc7230087ab1cdc65925be6d4cb/dblp http://www.bibsonomy.org/bibtex/25e432dc7230087ab1cdc65925be6d4cb/dblp 10.1186/1687-1499-2013-269<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1186/1687-1499-2013-269"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Statistical%20wavelet-based%20anomaly%20detection%20in%20big%20data%20with%20compressive%20sensing&amp;author=W.%20Wang&amp;author=D.%20Lu&amp;author=X.%20Zhou&amp;author=B.%20Zhang&amp;author=J.%20Mu&amp;journal=EURASIP%20J%20Wireless%20Commun%20Netw&amp;volume=2013&amp;pages=269&amp;publication_year=2013&amp;doi=10.1186%2F1687-1499-2013-269"><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation">Copyright information</h2><div class="ArticleCopyright content"><div class="ArticleCopyright">© Najafabadi et al.; licensee Springer. 2015<div class="CopyrightComment"><p class="SimplePara">This article is published under license to BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by/4.0"><span class="RefSource">http://creativecommons.org/licenses/by/4.0</span></a></span>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited.</p></div></div></div></section><section id="authorsandaffiliations" class="Section1 RenderAsSection1"><h2 class="Heading">Authors and Affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Maryam M Najafabadi</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Flavio Villanustre</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-2">2</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Taghi M Khoshgoftaar</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Naeem Seliya</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Randall Wald</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul><span class="author-information"><span class="author-information__contact u-icon-before icon--email-before"><a href="mailto:rwald1@fau.edu" title="rwald1@fau.edu" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Edin Muharemagic</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-3">3</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">Florida Atlantic University</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Boca Raton</span><span itemprop="addressCountry" class="affiliation__country">USA</span></span></span></li><li class="affiliation" data-test="affiliation-2" data-affiliation-highlight="affiliation-2" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">2.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">LexisNexis Business Information Solutions</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Atlanta</span><span itemprop="addressCountry" class="affiliation__country">USA</span></span></span></li><li class="affiliation" data-test="affiliation-3" data-affiliation-highlight="affiliation-3" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">3.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">LexisNexis Business Information Solutions</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Boca Raton</span><span itemprop="addressCountry" class="affiliation__country">USA</span></span></span></li></ol></div></section></div>
                        </article>
                        <aside class="section section--collapsible" id="AboutThisContent">
    <h2 class="section__heading" id="aboutcontent">About this article</h2>
    <div class="section__content bibliographic-information">
                <div id="crossMark" class="crossmark">
            <a data-crossmark="10.1186%2Fs40537-014-0007-7" class="gtm-crossmark" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186%2Fs40537-014-0007-7" title="Verify currency and authenticity via CrossMark">
                <span class="u-screenreader-only">CrossMark</span>
                <svg class="CrossMark" id="crossmark-icon" width="57" height="81">
                    <image width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="/springerlink-static/1645515599/images/png/crossmark.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/1645515599/images/svg/crossmark.svg"></image>
                </svg>
            </a>
        </div>

        <div class="crossmark__adjacent">
            <dl class="citation-info u-highlight-target u-mb-16" id="citeas" tabindex="-1">
    <dt class="test-cite-heading">
        Cite this article as:
    </dt>
    <dd id="citethis-text">Najafabadi, M.M., Villanustre, F., Khoshgoftaar, T.M. et al. Journal of Big Data (2015) 2: 1. https://doi.org/10.1186/s40537-014-0007-7</dd>
</dl>
                <ul class="bibliographic-information__list bibliographic-information__list--inline">
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">Received</span>
            <span class="bibliographic-information__value u-overflow-wrap">26 March 2014</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">Accepted</span>
            <span class="bibliographic-information__value u-overflow-wrap">14 August 2014</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">First Online</span>
            <span class="bibliographic-information__value u-overflow-wrap">24 February 2015</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">DOI</span>
            <span class="bibliographic-information__value u-overflow-wrap" id="doi-url">https://doi.org/10.1186/s40537-014-0007-7</span>
        </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Publisher Name</span>
                <span class="bibliographic-information__value" id="publisher-name">Springer International Publishing</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Online ISSN</span>
                <span class="bibliographic-information__value" id="electronic-issn">2196-1115</span>
            </li>

        
    </ul>

            <ul class="bibliographic-information__list">
        <li class="bibliographic-information__item">
            <a id="about-journal" class="bibliographic-information__misc-links gtm-about-this"
               title="Visit Springer.com for information about this article&#39;s journal"
               href="//www.springer.com/journal/40537/about">About this journal</a>
        </li>
</ul>
                <p class="u-mt-0" data-test="optional-article-license-text">
                    This article is published under an open access license.
                    Please check the 'Copyright Information' section for details of this license and
                    what re-use is permitted.
                    If your intended use exceeds what is permitted by the license or if
                    you are unable to locate the licence and re-use information,
                    please contact the <a href="mailto:journalpermissions@springernature.com">Rights and Permissions team</a>.
                </p>



        </div>
      
      
          
    </div>
</aside>

                        <div class="section section--collapsible uptodate-recommendations gtm-recommendations">
    <h2 class="uptodate-recommendations__title section__heading gtm-recommendations__title" id="uptodaterecommendations">Personalised recommendations</h2>
    <div class="section__content">
        <div class="uptodate-recommendations__container">
             <link rel="uptodate-inline" href="/springerlink-static/1645515599/css/recommendations.css"/>
        </div>
    </div>
</div>
                                <div id="doubleclick-native-ad" data-component="SpringerLink.GoogleAds" data-namespace="native"></div>

                                    <div class="sticky-banner 
            u-interface u-js-screenreader-only" aria-hidden="true" data-component="SpringerLink.StickyBanner" data-namespace="hasButton">
                <div class="sticky-banner__container">
                        <div class="citations" data-component="SV.Dropdown" data-namespace="citationsSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                            <div>
        <a class="c-button share-this gtm-shareby-sharelink-link test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="/sharelink/10.1186/s40537-014-0007-7">
            <span>Share</span>
            <span class="hide-text-small">article</span>
        </a>
    </div>




                                    <div>
            <a href="/content/pdf/10.1186%2Fs40537-014-0007-7.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

                </div>
            </div>




                    </div>
                    <aside class="main-sidebar-right u-interface">
                        <div data-role="sticky-wrapper">
                            <div class="main-sidebar-right__content u-composite-layer" data-component="SpringerLink.StickySidebar">
                                <div class="article-actions" id="article-actions">
                                    <h2 class="u-screenreader-only" aria-hidden="true">Actions</h2>


                                    <div class="u-js-hide u-js-show-two-col">
                                        

                                                <div class="download-article test-pdf-link">
                                                            <div>
            <a href="/content/pdf/10.1186%2Fs40537-014-0007-7.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

                                                </div>


                                            <div class="citations" data-component="SV.Dropdown" data-namespace="citations">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1186/s40537-014-0007-7?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                                <div>
        <a class="c-button share-this gtm-shareby-sharelink-link test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="/sharelink/10.1186/s40537-014-0007-7">
            <span>Share</span>
            <span class="hide-text-small">article</span>
        </a>
    </div>




                                    </div>
                                </div>
                                <nav class="toc" aria-label="article contents">
    <h2 class="u-screenreader-only" aria-hidden="true">Table of contents</h2>
    <ul id="article-contents" class="article-contents" role="menu" tabindex="-1">
            <li role="menuitem">
                <a title="Article" href="#enumeration"><span class="u-overflow-ellipsis">Article</span></a>
            </li>
            <li role="menuitem">
                <a title="Abstract" href="#Abs1"><span class="u-overflow-ellipsis">Abstract</span></a>
            </li>
            <li role="menuitem">
                <a title="Introduction" href="#Sec1"><span class="u-overflow-ellipsis">Introduction</span></a>
            </li>
            <li role="menuitem">
                <a title="Deep learning in data mining and machine learning" href="#Sec2"><span class="u-overflow-ellipsis">Deep learning in data mining and machine learning</span></a>
            </li>
            <li role="menuitem">
                <a title="Big data analytics" href="#Sec3"><span class="u-overflow-ellipsis">Big data analytics</span></a>
            </li>
            <li role="menuitem">
                <a title="Applications of deep learning in big data analytics" href="#Sec4"><span class="u-overflow-ellipsis">Applications of deep learning in big data analytics</span></a>
            </li>
            <li role="menuitem">
                <a title="Deep learning challenges in big data analytics" href="#Sec7"><span class="u-overflow-ellipsis">Deep learning challenges in big data analytics</span></a>
            </li>
            <li role="menuitem">
                <a title="Future work on deep learning in big data analytics" href="#Sec11"><span class="u-overflow-ellipsis">Future work on deep learning in big data analytics</span></a>
            </li>
            <li role="menuitem">
                <a title="Conclusion" href="#Sec12"><span class="u-overflow-ellipsis">Conclusion</span></a>
            </li>
            <li role="menuitem">
                <a title="Notes" href="#Notes"><span class="u-overflow-ellipsis">Notes</span></a>
            </li>
            <li role="menuitem">
                <a title="References" href="#Bib1"><span class="u-overflow-ellipsis">References</span></a>
            </li>
            <li role="menuitem">
                <a title="Copyright information" href="#copyrightInformation"><span class="u-overflow-ellipsis">Copyright information</span></a>
            </li>
            <li role="menuitem">
                <a title="Authors and Affiliations" href="#authorsandaffiliations"><span class="u-overflow-ellipsis">Authors and Affiliations</span></a>
            </li>
            <li role="menuitem">
                <a title="About this article" href="#aboutcontent"><span class="u-overflow-ellipsis">About this article</span></a>
            </li>
    </ul>
</nav>

                            </div>
                                <div class="skyscraper-ad u-hide" data-component="SpringerLink.GoogleAds" data-namespace="skyscraper">
        <div class="skyscraper-ad__wrapper">
            <p class="skyscraper-ad__label">Advertisement</p>
            <button class="skyscraper-ad__hide" title="Hide this advertisement">Hide</button>
            <div id="doubleclick-ad" class="skyscraper-ad__ad"></div>
        </div>
    </div>

                        </div>
                    </aside>
                </div>
            </main>
                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content">
                <div class="footer__aside">
                    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                                <div class="footer__edition" data-component="SV.EditionSwitcher">
                                    <h3 class="u-hide" data-role="button-dropdown__title" data-btn-text="Switch between Academic &#38; Corporate Edition">Switch Edition</h3>
                                    <ul data-role="button-dropdown__content">
                                        <li  class="selected"><a href="/siteEdition/link?previousUrl=/article/10.1186/s40537-014-0007-7&id=siteedition-academic-link" id="siteedition-academic-link">Academic Edition</a></li>
                                        <li ><a href="/siteEdition/rd?previousUrl=/article/10.1186/s40537-014-0007-7&id=siteedition-corporate-link" id="siteedition-corporate-link">Corporate Edition</a></li>
                                    </ul>
                                </div>
                </div>
            </div>
        </div>
        <div class="footer__content">
            <ul class="footer__nav">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/impressum">Impressum</a>
                </li>
                <li>
                    <a href="/termsandconditions">Legal information</a>
                </li>
                <li>
                    <a href="/privacystatement">Privacy statement</a>
                </li>
                <li>
                    <a href="/cookiepolicy">How we use cookies</a>
                </li>
                <li>
                    <a href="/accessibility" class="gtm-footer-accessibility">Accessibility</a>
                </li>
                <li>
                    <a id="contactus-footer-link" href="/contactus">Contact us</a>
                </li>
            </ul>
            <a class="parent-logo"
               target="_blank" rel="noopener"
               href="//www.springernature.com"
               title="Go to Springer Nature">
                <span class="u-screenreader-only">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12"
                           src="/springerlink-static/1645515599/images/png/springernature.png"
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href="/springerlink-static/1645515599/images/svg/springernature.svg">
                    </image>
                </svg>
            </a>

            <p class="footer__copyright">&copy; 2018 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
                <p class="footer__user-access-info">
                    <span>Not logged in</span>
                    <span>Not affiliated</span>
                    <span>104.32.181.101</span>
                </p>
        </div>
    </footer>

        </div>
        <script type="text/javascript">
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        var scriptsList = [];
        var polyfillFeatures = '';

        window.SpringerLink = window.SpringerLink || {};
        window.SpringerLink.staticLocation = '/springerlink-static/1645515599';
        window.eventTrackerInstance = null;

        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);

            polyfillFeatures = 'default,fetch,Promise,Object.setPrototypeOf,Object.entries,Number.isInteger,MutationObserver,startsWith,Array.prototype.includes,Array.from,IntersectionObserver';

            scriptsList = [
                'https://cdn.polyfill.io/v2/polyfill.min.js?features=' + polyfillFeatures + '&flags=gated',
                window.SpringerLink.staticLocation + '/js/main.js'
            ];

            scriptsList.forEach(function(script) {
                var tag = document.createElement('script');
                tag.async = false;
                tag.src = script;

                document.body.appendChild(tag);
            });
        }
    })();
</script>



    <script type="text/javascript" id="googletag-push">
        
            var adSlot = '270604982/springerlink/40537/article';
        

        var definedSlots = [
                {slot: [728, 90], containerName: 'doubleclick-leaderboard-ad'},
                {slot: [160, 600], containerName: 'doubleclick-ad'},
            {slot: [2, 2], containerName: 'doubleclick-native-ad'}
        ];
    </script>


        
        <span id="chat-widget" class="u-hide"></span>
                    <noscript>
                <img aria-hidden="true" role="presentation" src="https://ssl-springer.met.vgwort.de/na/vgzm.415900-10.1186-s40537-014-0007-7" width='1' height='1' alt='' />
            </noscript>

    </body>
</html>
